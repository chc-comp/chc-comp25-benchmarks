(set-logic HORN)


(declare-fun |step_lturn| ( (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) ) Bool)
(declare-fun |lturn| ( (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) ) Bool)
(declare-fun |CHC_COMP_FALSE| ( ) Bool)
(declare-fun |combined_lturn| ( (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) ) Bool)

(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) ) 
    (=>
      (and
        (and (bvsle #x00000000 (bvadd #xfffffff8 C I))
     (bvsle #x00000000 (bvadd #xfffffff8 B H))
     (bvsle #x00000000 (bvadd #xfffffff8 K C))
     (bvsle #x00000000 (bvadd #xfffffff7 F I))
     (bvsle #x00000000 (bvadd #xfffffff7 F K))
     (bvsle #x00000000 (bvadd #xfffffff7 D I))
     (bvsle #x00000000 (bvadd #xfffffff7 C H))
     (bvsle #x00000000 (bvadd #xfffffff7 B C))
     (bvsle #x00000000 (bvadd #xfffffff7 K D))
     (bvsle #x00000000 (bvadd #xfffffff2 E G))
     (bvsle #x00000000 (bvadd #xfffffff2 E A))
     (bvsle #x00000000 (bvadd #xfffffff2 A G))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff C) G))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff B) D))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff K) D))
     (bvsle #x00000000 (bvadd #xfffffffe F (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffe F (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #xfffffffe F (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xfffffffe E (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd #xfffffffe D (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffe C (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffe A (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd #xfffffff4 F D))
     (bvsle #x00000000 (bvadd #xfffffff4 E C))
     (bvsle #x00000000 (bvadd #xfffffff4 C G))
     (bvsle #x00000000 (bvadd #xfffffff4 A C))
     (bvsle #x00000000 (bvadd #xfffffff3 F G))
     (bvsle #x00000000 (bvadd #xfffffff3 F A))
     (bvsle #x00000000 (bvadd #xfffffff3 E F))
     (bvsle #x00000000 (bvadd #xfffffff3 E D))
     (bvsle #x00000000 (bvadd #xfffffff3 D G))
     (bvsle #x00000000 (bvadd #xfffffff3 A D))
     (bvsle #x00000000 (bvadd #xfffffffc (bvmul #xffffffff I) G))
     (bvsle #x00000000 (bvadd #xfffffffc E (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffc A (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffff6 I G))
     (bvsle #x00000000 (bvadd #xfffffff6 F H))
     (bvsle #x00000000 (bvadd #xfffffff6 F B))
     (bvsle #x00000000 (bvadd #xfffffff6 E I))
     (bvsle #x00000000 (bvadd #xfffffff6 E K))
     (bvsle #x00000000 (bvadd #xfffffff6 D H))
     (bvsle #x00000000 (bvadd #xfffffff6 B D))
     (bvsle #x00000000 (bvadd #xfffffff6 A I))
     (bvsle #x00000000 (bvadd #xfffffff6 A K))
     (bvsle #x00000000 (bvadd #xfffffff6 K G))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff H) G))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff B) G))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff K) G))
     (bvsle #x00000000 (bvadd #xfffffffd F (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffd E (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffd E (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #xfffffffd E (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xfffffffd D (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffd A (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffd A (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #xfffffffd A (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff I) H))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff F) G))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff F) A))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff D) G))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff C) D))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff B) C))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff K) C))
     (bvsle #x00000000 (bvadd #xffffffff F (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd #xffffffff E (bvmul #xffffffff D)))
     (bvsle #x00000000 (bvadd #xffffffff C (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xffffffff B (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xffffffff A (bvmul #xffffffff D)))
     (bvsle #x00000000 (bvadd #xfffffff5 H G))
     (bvsle #x00000000 (bvadd #xfffffff5 F C))
     (bvsle #x00000000 (bvadd #xfffffff5 E H))
     (bvsle #x00000000 (bvadd #xfffffff5 E B))
     (bvsle #x00000000 (bvadd #xfffffff5 C D))
     (bvsle #x00000000 (bvadd #xfffffff5 B G))
     (bvsle #x00000000 (bvadd #xfffffff5 A H))
     (bvsle #x00000000 (bvadd #xfffffff5 A B))
     (bvsle #x00000000 (bvadd #xfffffff9 I H))
     (bvsle #x00000000 (bvadd #xfffffff9 B I))
     (bvsle #x00000000 (bvadd #xfffffff9 K H))
     (bvsle #x00000000 (bvadd #xfffffff9 K B))
     (bvsle #x00000000 (bvadd #x00000002 (bvmul #xffffffff E) C))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff E) F))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff E) D))
     (bvsle #x00000000 (bvadd #x00000001 C (bvmul #xffffffff D)))
     (bvsle #x00000000 (bvadd #xfffffffb C))
     (bvsle #x00000000 (bvadd #xfffffffa F))
     (bvsle #x00000000 (bvadd #xfffffffa D))
     (bvsle #x00000000 (bvadd #xfffffffc H))
     (bvsle #x00000000 (bvadd #xfffffffc B))
     (bvsle #x00000000 (bvadd #xfffffffd I))
     (bvsle #x00000000 (bvadd #xfffffffd K))
     (bvsle #x00000000 (bvadd #xfffffff9 G))
     (bvsle #x00000000 (bvadd #xfffffff9 E))
     (bvsle #x00000000 (bvadd #xfffffff9 A))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff E) G))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff E) A))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff B) H))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff A) G))
     (bvsle #x00000000 (bvadd F (bvmul #xffffffff D)))
     (bvsle #x00000000 (bvadd B (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd A (bvmul #xffffffff G)))
     (bvsle #x00000000 (bvadd K (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffa K I)))
      )
      (lturn A B C D K E F G H I J)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) ) 
    (=>
      (and
        (and (bvsle #x00000000 (bvadd #xfffffff0 E A))
     (bvsle #x00000000 (bvadd #xfffffff0 A G))
     (bvsle #x00000000 (bvadd #xfffffff1 F G))
     (bvsle #x00000000 (bvadd #xfffffff1 F A))
     (bvsle #x00000000 (bvadd #xfffffff1 E F))
     (bvsle #x00000000 (bvadd #xfffffff1 E D))
     (bvsle #x00000000 (bvadd #xfffffff1 D G))
     (bvsle #x00000000 (bvadd #xfffffff1 A D))
     (bvsle #x00000000 (bvadd #xfffffffb (bvmul #xffffffff I) G))
     (bvsle #x00000000 (bvadd #xfffffffb E (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffb A (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffa K I))
     (bvsle #x00000000 (bvadd #xfffffff8 B I))
     (bvsle #x00000000 (bvadd #xfffffff8 K B))
     (bvsle #x00000000 (bvadd #xfffffff7 C I))
     (bvsle #x00000000 (bvadd #xfffffff7 B H))
     (bvsle #x00000000 (bvadd #xfffffff7 K C))
     (bvsle #x00000000 (bvadd #xfffffff2 F D))
     (bvsle #x00000000 (bvadd #xfffffff2 E C))
     (bvsle #x00000000 (bvadd #xfffffff2 C G))
     (bvsle #x00000000 (bvadd #xfffffff2 A C))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff C) G))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff B) D))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff K) D))
     (bvsle #x00000000 (bvadd #xfffffffe F (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #xfffffffe F (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xfffffffe E (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd #xfffffffe C (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffe B (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffe A (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd #xfffffff4 H G))
     (bvsle #x00000000 (bvadd #xfffffff4 F B))
     (bvsle #x00000000 (bvadd #xfffffff4 E H))
     (bvsle #x00000000 (bvadd #xfffffff4 B D))
     (bvsle #x00000000 (bvadd #xfffffff4 A H))
     (bvsle #x00000000 (bvadd #xfffffff3 F C))
     (bvsle #x00000000 (bvadd #xfffffff3 E B))
     (bvsle #x00000000 (bvadd #xfffffff3 C D))
     (bvsle #x00000000 (bvadd #xfffffff3 B G))
     (bvsle #x00000000 (bvadd #xfffffff3 A B))
     (bvsle #x00000000 (bvadd #xfffffffc (bvmul #xffffffff H) G))
     (bvsle #x00000000 (bvadd #xfffffffc F (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffc E (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffc D (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffc A (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffff6 F I))
     (bvsle #x00000000 (bvadd #xfffffff6 F K))
     (bvsle #x00000000 (bvadd #xfffffff6 D I))
     (bvsle #x00000000 (bvadd #xfffffff6 C H))
     (bvsle #x00000000 (bvadd #xfffffff6 K D))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff B) G))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff K) G))
     (bvsle #x00000000 (bvadd #xfffffffd F (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffd E (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #xfffffffd E (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xfffffffd D (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffd C (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffd A (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #xfffffffd A (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff I) H))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff F) G))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff F) A))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff D) G))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff C) D))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff B) C))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff K) C))
     (bvsle #x00000000 (bvadd #xffffffff F (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd #xffffffff E (bvmul #xffffffff D)))
     (bvsle #x00000000 (bvadd #xffffffff B (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xffffffff A (bvmul #xffffffff D)))
     (bvsle #x00000000 (bvadd #xfffffff5 I G))
     (bvsle #x00000000 (bvadd #xfffffff5 F H))
     (bvsle #x00000000 (bvadd #xfffffff5 E I))
     (bvsle #x00000000 (bvadd #xfffffff5 E K))
     (bvsle #x00000000 (bvadd #xfffffff5 D H))
     (bvsle #x00000000 (bvadd #xfffffff5 B C))
     (bvsle #x00000000 (bvadd #xfffffff5 A I))
     (bvsle #x00000000 (bvadd #xfffffff5 A K))
     (bvsle #x00000000 (bvadd #xfffffff5 K G))
     (bvsle #x00000000 (bvadd #xfffffff9 I H))
     (bvsle #x00000000 (bvadd #xfffffff9 K H))
     (bvsle #x00000000 (bvadd #x00000002 (bvmul #xffffffff E) C))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff E) F))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff E) D))
     (bvsle #x00000000 (bvadd #x00000001 C (bvmul #xffffffff D)))
     (bvsle #x00000000 (bvadd #xfffffffb B))
     (bvsle #x00000000 (bvadd #xfffffffa C))
     (bvsle #x00000000 (bvadd #xfffffff8 G))
     (bvsle #x00000000 (bvadd #xfffffff8 E))
     (bvsle #x00000000 (bvadd #xfffffff8 A))
     (bvsle #x00000000 (bvadd #xfffffffc H))
     (bvsle #x00000000 (bvadd #xfffffffd I))
     (bvsle #x00000000 (bvadd #xfffffffd K))
     (bvsle #x00000000 (bvadd #xfffffff9 F))
     (bvsle #x00000000 (bvadd #xfffffff9 D))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff E) G))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff E) A))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff A) G))
     (bvsle #x00000000 (bvadd F (bvmul #xffffffff D)))
     (bvsle #x00000000 (bvadd A (bvmul #xffffffff G)))
     (bvsle #x00000000 (bvadd K (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffff0 E G)))
      )
      (lturn A B C D K E F G H I J)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) ) 
    (=>
      (and
        (and (bvsle #x00000000 (bvadd #xfffffffa B I))
     (bvsle #x00000000 (bvadd #xfffffffa K H))
     (bvsle #x00000000 (bvadd #xfffffffa K B))
     (bvsle #x00000000 (bvadd #xfffffff8 F H))
     (bvsle #x00000000 (bvadd #xfffffff8 F B))
     (bvsle #x00000000 (bvadd #xfffffff8 D H))
     (bvsle #x00000000 (bvadd #xfffffff8 B D))
     (bvsle #x00000000 (bvadd #xfffffff8 K I))
     (bvsle #x00000000 (bvadd #xfffffff7 H G))
     (bvsle #x00000000 (bvadd #xfffffff7 E H))
     (bvsle #x00000000 (bvadd #xfffffff7 E B))
     (bvsle #x00000000 (bvadd #xfffffff7 C I))
     (bvsle #x00000000 (bvadd #xfffffff7 B G))
     (bvsle #x00000000 (bvadd #xfffffff7 A H))
     (bvsle #x00000000 (bvadd #xfffffff7 A B))
     (bvsle #x00000000 (bvadd #xfffffff7 K C))
     (bvsle #x00000000 (bvadd #xfffffff2 E G))
     (bvsle #x00000000 (bvadd #xfffffff2 E A))
     (bvsle #x00000000 (bvadd #xfffffff2 A G))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff C) G))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff B) C))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff K) D))
     (bvsle #x00000000 (bvadd #xfffffffe F (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffe F (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xfffffffe E (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd #xfffffffe D (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffe C (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffe A (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd #xfffffff4 F D))
     (bvsle #x00000000 (bvadd #xfffffff4 E C))
     (bvsle #x00000000 (bvadd #xfffffff4 C G))
     (bvsle #x00000000 (bvadd #xfffffff4 A C))
     (bvsle #x00000000 (bvadd #xfffffff3 F G))
     (bvsle #x00000000 (bvadd #xfffffff3 F A))
     (bvsle #x00000000 (bvadd #xfffffff3 E F))
     (bvsle #x00000000 (bvadd #xfffffff3 E D))
     (bvsle #x00000000 (bvadd #xfffffff3 D G))
     (bvsle #x00000000 (bvadd #xfffffff3 A D))
     (bvsle #x00000000 (bvadd #xfffffffc (bvmul #xffffffff H) G))
     (bvsle #x00000000 (bvadd #xfffffffc (bvmul #xffffffff B) G))
     (bvsle #x00000000 (bvadd #xfffffffc E (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffc E (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #xfffffffc B H))
     (bvsle #x00000000 (bvadd #xfffffffc A (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffc A (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #xfffffff6 F I))
     (bvsle #x00000000 (bvadd #xfffffff6 F K))
     (bvsle #x00000000 (bvadd #xfffffff6 D I))
     (bvsle #x00000000 (bvadd #xfffffff6 K D))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff I) G))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff B) D))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff K) G))
     (bvsle #x00000000 (bvadd #xfffffffd F (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffd F (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #xfffffffd E (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffd E (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xfffffffd D (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffd A (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffd A (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff F) G))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff F) A))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff D) G))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff C) D))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff B) I))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff K) C))
     (bvsle #x00000000 (bvadd #xffffffff I (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xffffffff F (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd #xffffffff E (bvmul #xffffffff D)))
     (bvsle #x00000000 (bvadd #xffffffff C (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xffffffff A (bvmul #xffffffff D)))
     (bvsle #x00000000 (bvadd #xffffffff K (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xffffffff K (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #xfffffff5 I G))
     (bvsle #x00000000 (bvadd #xfffffff5 F C))
     (bvsle #x00000000 (bvadd #xfffffff5 E I))
     (bvsle #x00000000 (bvadd #xfffffff5 E K))
     (bvsle #x00000000 (bvadd #xfffffff5 C D))
     (bvsle #x00000000 (bvadd #xfffffff5 A I))
     (bvsle #x00000000 (bvadd #xfffffff5 A K))
     (bvsle #x00000000 (bvadd #xfffffff5 K G))
     (bvsle #x00000000 (bvadd #xfffffff9 C H))
     (bvsle #x00000000 (bvadd #xfffffff9 B C))
     (bvsle #x00000000 (bvadd #x00000002 (bvmul #xffffffff E) C))
     (bvsle #x00000000 (bvadd #x00000002 K (bvmul #xffffffff D)))
     (bvsle #x00000000 (bvadd #x00000003 (bvmul #xffffffff E) K))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff E) F))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff E) D))
     (bvsle #x00000000 (bvadd #x00000001 C (bvmul #xffffffff D)))
     (bvsle #x00000000 (bvadd #x00000001 K (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd #xfffffffb C))
     (bvsle #x00000000 (bvadd #xfffffffa F))
     (bvsle #x00000000 (bvadd #xfffffffa D))
     (bvsle #x00000000 (bvadd #xfffffffe H))
     (bvsle #x00000000 (bvadd #xfffffffe B))
     (bvsle #x00000000 (bvadd #xfffffffc I))
     (bvsle #x00000000 (bvadd #xfffffffc K))
     (bvsle #x00000000 (bvadd #xfffffff9 G))
     (bvsle #x00000000 (bvadd #xfffffff9 E))
     (bvsle #x00000000 (bvadd #xfffffff9 A))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff E) G))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff E) A))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff B) H))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff A) G))
     (bvsle #x00000000 (bvadd F (bvmul #xffffffff D)))
     (bvsle #x00000000 (bvadd B (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd A (bvmul #xffffffff G)))
     (bvsle #x00000000 (bvadd K (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffa I H)))
      )
      (lturn A B C D K E F G H I J)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) ) 
    (=>
      (and
        (and (bvsle #x00000000 (bvadd #xfffffff0 E A))
     (bvsle #x00000000 (bvadd #xfffffff0 A G))
     (bvsle #x00000000 (bvadd #xfffffff1 F G))
     (bvsle #x00000000 (bvadd #xfffffff1 F A))
     (bvsle #x00000000 (bvadd #xfffffff1 E F))
     (bvsle #x00000000 (bvadd #xfffffff1 E D))
     (bvsle #x00000000 (bvadd #xfffffff1 D G))
     (bvsle #x00000000 (bvadd #xfffffff1 A D))
     (bvsle #x00000000 (bvadd #xfffffffb (bvmul #xffffffff H) G))
     (bvsle #x00000000 (bvadd #xfffffffb E (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffb A (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffa I H))
     (bvsle #x00000000 (bvadd #xfffffffa K H))
     (bvsle #x00000000 (bvadd #xfffffff8 C H))
     (bvsle #x00000000 (bvadd #xfffffff8 K I))
     (bvsle #x00000000 (bvadd #xfffffff7 F H))
     (bvsle #x00000000 (bvadd #xfffffff7 D H))
     (bvsle #x00000000 (bvadd #xfffffff7 B I))
     (bvsle #x00000000 (bvadd #xfffffff7 K B))
     (bvsle #x00000000 (bvadd #xfffffff2 F D))
     (bvsle #x00000000 (bvadd #xfffffff2 E C))
     (bvsle #x00000000 (bvadd #xfffffff2 C G))
     (bvsle #x00000000 (bvadd #xfffffff2 A C))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff C) G))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff B) D))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff K) D))
     (bvsle #x00000000 (bvadd #xfffffffe F (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #xfffffffe F (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xfffffffe E (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd #xfffffffe C (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffe B (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffe A (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd #xfffffff4 I G))
     (bvsle #x00000000 (bvadd #xfffffff4 F B))
     (bvsle #x00000000 (bvadd #xfffffff4 E I))
     (bvsle #x00000000 (bvadd #xfffffff4 E K))
     (bvsle #x00000000 (bvadd #xfffffff4 B D))
     (bvsle #x00000000 (bvadd #xfffffff4 A I))
     (bvsle #x00000000 (bvadd #xfffffff4 A K))
     (bvsle #x00000000 (bvadd #xfffffff4 K G))
     (bvsle #x00000000 (bvadd #xfffffff3 F C))
     (bvsle #x00000000 (bvadd #xfffffff3 E B))
     (bvsle #x00000000 (bvadd #xfffffff3 C D))
     (bvsle #x00000000 (bvadd #xfffffff3 B G))
     (bvsle #x00000000 (bvadd #xfffffff3 A B))
     (bvsle #x00000000 (bvadd #xfffffffc (bvmul #xffffffff I) G))
     (bvsle #x00000000 (bvadd #xfffffffc F (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffc E (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffc D (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffc A (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffff6 H G))
     (bvsle #x00000000 (bvadd #xfffffff6 E H))
     (bvsle #x00000000 (bvadd #xfffffff6 C I))
     (bvsle #x00000000 (bvadd #xfffffff6 A H))
     (bvsle #x00000000 (bvadd #xfffffff6 K C))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff B) G))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff K) G))
     (bvsle #x00000000 (bvadd #xfffffffd F (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffd E (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #xfffffffd E (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xfffffffd D (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffd C (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffd A (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #xfffffffd A (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff F) G))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff F) A))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff D) G))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff C) D))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff B) C))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff K) C))
     (bvsle #x00000000 (bvadd #xffffffff I (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xffffffff F (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd #xffffffff E (bvmul #xffffffff D)))
     (bvsle #x00000000 (bvadd #xffffffff B (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xffffffff A (bvmul #xffffffff D)))
     (bvsle #x00000000 (bvadd #xffffffff K (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffff5 F I))
     (bvsle #x00000000 (bvadd #xfffffff5 F K))
     (bvsle #x00000000 (bvadd #xfffffff5 D I))
     (bvsle #x00000000 (bvadd #xfffffff5 B C))
     (bvsle #x00000000 (bvadd #xfffffff5 K D))
     (bvsle #x00000000 (bvadd #xfffffff9 B H))
     (bvsle #x00000000 (bvadd #x00000002 (bvmul #xffffffff E) C))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff E) F))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff E) D))
     (bvsle #x00000000 (bvadd #x00000001 C (bvmul #xffffffff D)))
     (bvsle #x00000000 (bvadd #xfffffffb B))
     (bvsle #x00000000 (bvadd #xfffffffa C))
     (bvsle #x00000000 (bvadd #xfffffff8 G))
     (bvsle #x00000000 (bvadd #xfffffff8 E))
     (bvsle #x00000000 (bvadd #xfffffff8 A))
     (bvsle #x00000000 (bvadd #xfffffffe H))
     (bvsle #x00000000 (bvadd #xfffffffc I))
     (bvsle #x00000000 (bvadd #xfffffffc K))
     (bvsle #x00000000 (bvadd #xfffffff9 F))
     (bvsle #x00000000 (bvadd #xfffffff9 D))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff E) G))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff E) A))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff A) G))
     (bvsle #x00000000 (bvadd F (bvmul #xffffffff D)))
     (bvsle #x00000000 (bvadd A (bvmul #xffffffff G)))
     (bvsle #x00000000 (bvadd K (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffff0 E G)))
      )
      (lturn A B C D K E F G H I J)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) ) 
    (=>
      (and
        (and (bvsle #x00000000 (bvadd #xfffffffb B I))
     (bvsle #x00000000 (bvadd #xfffffffb K H))
     (bvsle #x00000000 (bvadd #xfffffffb K B))
     (bvsle #x00000000 (bvadd #xfffffffa C H))
     (bvsle #x00000000 (bvadd #xfffffffa B C))
     (bvsle #x00000000 (bvadd #xfffffffa K I))
     (bvsle #x00000000 (bvadd #xfffffff8 H G))
     (bvsle #x00000000 (bvadd #xfffffff8 F I))
     (bvsle #x00000000 (bvadd #xfffffff8 F K))
     (bvsle #x00000000 (bvadd #xfffffff8 E H))
     (bvsle #x00000000 (bvadd #xfffffff8 E B))
     (bvsle #x00000000 (bvadd #xfffffff8 D I))
     (bvsle #x00000000 (bvadd #xfffffff8 B G))
     (bvsle #x00000000 (bvadd #xfffffff8 A H))
     (bvsle #x00000000 (bvadd #xfffffff8 A B))
     (bvsle #x00000000 (bvadd #xfffffff8 K D))
     (bvsle #x00000000 (bvadd #xfffffff7 I G))
     (bvsle #x00000000 (bvadd #xfffffff7 F C))
     (bvsle #x00000000 (bvadd #xfffffff7 E I))
     (bvsle #x00000000 (bvadd #xfffffff7 E K))
     (bvsle #x00000000 (bvadd #xfffffff7 C D))
     (bvsle #x00000000 (bvadd #xfffffff7 A I))
     (bvsle #x00000000 (bvadd #xfffffff7 A K))
     (bvsle #x00000000 (bvadd #xfffffff7 K G))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff C) G))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff B) C))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff K) D))
     (bvsle #x00000000 (bvadd #xfffffffe F (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffe F (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xfffffffe E (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd #xfffffffe D (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffe C (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffe A (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd #xfffffff4 E G))
     (bvsle #x00000000 (bvadd #xfffffff4 E A))
     (bvsle #x00000000 (bvadd #xfffffff4 A G))
     (bvsle #x00000000 (bvadd #xfffffffc (bvmul #xffffffff H) G))
     (bvsle #x00000000 (bvadd #xfffffffc (bvmul #xffffffff B) G))
     (bvsle #x00000000 (bvadd #xfffffffc E (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffc E (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #xfffffffc B H))
     (bvsle #x00000000 (bvadd #xfffffffc A (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffc A (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #xfffffff6 F D))
     (bvsle #x00000000 (bvadd #xfffffff6 E C))
     (bvsle #x00000000 (bvadd #xfffffff6 C G))
     (bvsle #x00000000 (bvadd #xfffffff6 A C))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff I) G))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff B) D))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff K) G))
     (bvsle #x00000000 (bvadd #xfffffffd F (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffd F (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #xfffffffd E (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffd E (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xfffffffd D (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffd A (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffd A (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff F) G))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff F) A))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff D) G))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff C) D))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff B) I))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff K) C))
     (bvsle #x00000000 (bvadd #xffffffff I (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xffffffff F (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd #xffffffff E (bvmul #xffffffff D)))
     (bvsle #x00000000 (bvadd #xffffffff C (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xffffffff A (bvmul #xffffffff D)))
     (bvsle #x00000000 (bvadd #xffffffff K (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xffffffff K (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #xfffffff5 F G))
     (bvsle #x00000000 (bvadd #xfffffff5 F A))
     (bvsle #x00000000 (bvadd #xfffffff5 E F))
     (bvsle #x00000000 (bvadd #xfffffff5 E D))
     (bvsle #x00000000 (bvadd #xfffffff5 D G))
     (bvsle #x00000000 (bvadd #xfffffff5 A D))
     (bvsle #x00000000 (bvadd #xfffffff9 F H))
     (bvsle #x00000000 (bvadd #xfffffff9 F B))
     (bvsle #x00000000 (bvadd #xfffffff9 D H))
     (bvsle #x00000000 (bvadd #xfffffff9 C I))
     (bvsle #x00000000 (bvadd #xfffffff9 B D))
     (bvsle #x00000000 (bvadd #xfffffff9 K C))
     (bvsle #x00000000
            (bvadd #x00000005 (bvmul #xffffffff I) (bvmul #xffffffff H)))
     (bvsle #x00000000
            (bvadd #x00000005 (bvmul #xffffffff B) (bvmul #xffffffff I)))
     (bvsle #x00000000
            (bvadd #x00000004 (bvmul #xffffffff B) (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #x00000002 (bvmul #xffffffff E) C))
     (bvsle #x00000000 (bvadd #x00000002 K (bvmul #xffffffff D)))
     (bvsle #x00000000 (bvadd #x00000003 (bvmul #xffffffff E) K))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff I) H))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff E) F))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff E) D))
     (bvsle #x00000000 (bvadd #x00000001 C (bvmul #xffffffff D)))
     (bvsle #x00000000 (bvadd #x00000001 B (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #x00000001 K (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd #xfffffffb F))
     (bvsle #x00000000 (bvadd #xfffffffb D))
     (bvsle #x00000000 (bvadd #xfffffffa G))
     (bvsle #x00000000 (bvadd #xfffffffa E))
     (bvsle #x00000000 (bvadd #xfffffffa A))
     (bvsle #x00000000 (bvadd #xfffffffe H))
     (bvsle #x00000000 (bvadd #xfffffffe B))
     (bvsle #x00000000 (bvadd #xfffffffc C))
     (bvsle #x00000000 (bvadd #xfffffffd I))
     (bvsle #x00000000 (bvadd #xfffffffd K))
     (bvsle #x00000000 (bvadd #x00000002 (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #x00000002 (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #x00000003 (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff E) G))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff E) A))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff B) H))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff A) G))
     (bvsle #x00000000 (bvadd F (bvmul #xffffffff D)))
     (bvsle #x00000000 (bvadd B (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd A (bvmul #xffffffff G)))
     (bvsle #x00000000 (bvadd K (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffb I H)))
      )
      (lturn A B C D K E F G H I J)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) ) 
    (=>
      (and
        (and (bvsle #x00000000 (bvadd #xfffffffb I H))
     (bvsle #x00000000 (bvadd #xfffffffb E (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffb A (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffb K H))
     (bvsle #x00000000 (bvadd #xfffffffa B H))
     (bvsle #x00000000 (bvadd #xfffffffa K I))
     (bvsle #x00000000 (bvadd #xfffffff8 F H))
     (bvsle #x00000000 (bvadd #xfffffff8 D H))
     (bvsle #x00000000 (bvadd #xfffffff8 C I))
     (bvsle #x00000000 (bvadd #xfffffff8 K C))
     (bvsle #x00000000 (bvadd #xfffffff7 H G))
     (bvsle #x00000000 (bvadd #xfffffff7 F I))
     (bvsle #x00000000 (bvadd #xfffffff7 F K))
     (bvsle #x00000000 (bvadd #xfffffff7 E H))
     (bvsle #x00000000 (bvadd #xfffffff7 D I))
     (bvsle #x00000000 (bvadd #xfffffff7 B C))
     (bvsle #x00000000 (bvadd #xfffffff7 A H))
     (bvsle #x00000000 (bvadd #xfffffff7 K D))
     (bvsle #x00000000 (bvadd #xfffffff2 E G))
     (bvsle #x00000000 (bvadd #xfffffff2 E A))
     (bvsle #x00000000 (bvadd #xfffffff2 A G))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff C) G))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff B) D))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff K) D))
     (bvsle #x00000000 (bvadd #xfffffffe F (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #xfffffffe F (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xfffffffe E (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd #xfffffffe C (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffe B (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffe A (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd #xfffffff4 F D))
     (bvsle #x00000000 (bvadd #xfffffff4 E C))
     (bvsle #x00000000 (bvadd #xfffffff4 C G))
     (bvsle #x00000000 (bvadd #xfffffff4 A C))
     (bvsle #x00000000 (bvadd #xfffffff3 F G))
     (bvsle #x00000000 (bvadd #xfffffff3 F A))
     (bvsle #x00000000 (bvadd #xfffffff3 E F))
     (bvsle #x00000000 (bvadd #xfffffff3 E D))
     (bvsle #x00000000 (bvadd #xfffffff3 D G))
     (bvsle #x00000000 (bvadd #xfffffff3 A D))
     (bvsle #x00000000 (bvadd #xfffffffc (bvmul #xffffffff I) G))
     (bvsle #x00000000 (bvadd #xfffffffc F (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffc E (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffc D (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffc A (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffff6 I G))
     (bvsle #x00000000 (bvadd #xfffffff6 F B))
     (bvsle #x00000000 (bvadd #xfffffff6 E I))
     (bvsle #x00000000 (bvadd #xfffffff6 E K))
     (bvsle #x00000000 (bvadd #xfffffff6 B D))
     (bvsle #x00000000 (bvadd #xfffffff6 A I))
     (bvsle #x00000000 (bvadd #xfffffff6 A K))
     (bvsle #x00000000 (bvadd #xfffffff6 K G))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff B) G))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff K) G))
     (bvsle #x00000000 (bvadd #xfffffffd F (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffd E (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #xfffffffd E (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xfffffffd D (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffd C (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffd A (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #xfffffffd A (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff F) G))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff F) A))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff D) G))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff C) D))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff B) C))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff K) C))
     (bvsle #x00000000 (bvadd #xffffffff I (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xffffffff F (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd #xffffffff E (bvmul #xffffffff D)))
     (bvsle #x00000000 (bvadd #xffffffff B (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xffffffff A (bvmul #xffffffff D)))
     (bvsle #x00000000 (bvadd #xffffffff K (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffff5 F C))
     (bvsle #x00000000 (bvadd #xfffffff5 E B))
     (bvsle #x00000000 (bvadd #xfffffff5 C D))
     (bvsle #x00000000 (bvadd #xfffffff5 B G))
     (bvsle #x00000000 (bvadd #xfffffff5 A B))
     (bvsle #x00000000 (bvadd #xfffffff9 C H))
     (bvsle #x00000000 (bvadd #xfffffff9 B I))
     (bvsle #x00000000 (bvadd #xfffffff9 K B))
     (bvsle #x00000000
            (bvadd #x00000005 (bvmul #xffffffff I) (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #x00000002 (bvmul #xffffffff E) C))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff I) H))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff E) F))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff E) D))
     (bvsle #x00000000 (bvadd #x00000001 C (bvmul #xffffffff D)))
     (bvsle #x00000000 (bvadd #xfffffffb C))
     (bvsle #x00000000 (bvadd #xfffffffa F))
     (bvsle #x00000000 (bvadd #xfffffffa D))
     (bvsle #x00000000 (bvadd #xfffffffe H))
     (bvsle #x00000000 (bvadd #xfffffffc B))
     (bvsle #x00000000 (bvadd #xfffffffd I))
     (bvsle #x00000000 (bvadd #xfffffffd K))
     (bvsle #x00000000 (bvadd #xfffffff9 G))
     (bvsle #x00000000 (bvadd #xfffffff9 E))
     (bvsle #x00000000 (bvadd #xfffffff9 A))
     (bvsle #x00000000 (bvadd #x00000002 (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #x00000003 (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff E) G))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff E) A))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff A) G))
     (bvsle #x00000000 (bvadd F (bvmul #xffffffff D)))
     (bvsle #x00000000 (bvadd A (bvmul #xffffffff G)))
     (bvsle #x00000000 (bvadd K (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffb (bvmul #xffffffff H) G)))
      )
      (lturn A B C D K E F G H I J)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) ) 
    (=>
      (and
        (and (bvsle #x00000000 (bvadd #xfffffffb K B))
     (bvsle #x00000000 (bvadd #xfffffffa I H))
     (bvsle #x00000000 (bvadd #xfffffffa C H))
     (bvsle #x00000000 (bvadd #xfffffffa B I))
     (bvsle #x00000000 (bvadd #xfffffffa B C))
     (bvsle #x00000000 (bvadd #xfffffff8 H G))
     (bvsle #x00000000 (bvadd #xfffffff8 F K))
     (bvsle #x00000000 (bvadd #xfffffff8 E H))
     (bvsle #x00000000 (bvadd #xfffffff8 E B))
     (bvsle #x00000000 (bvadd #xfffffff8 C I))
     (bvsle #x00000000 (bvadd #xfffffff8 B G))
     (bvsle #x00000000 (bvadd #xfffffff8 A H))
     (bvsle #x00000000 (bvadd #xfffffff8 A B))
     (bvsle #x00000000 (bvadd #xfffffff8 K D))
     (bvsle #x00000000 (bvadd #xfffffff7 F I))
     (bvsle #x00000000 (bvadd #xfffffff7 F C))
     (bvsle #x00000000 (bvadd #xfffffff7 E K))
     (bvsle #x00000000 (bvadd #xfffffff7 D I))
     (bvsle #x00000000 (bvadd #xfffffff7 C D))
     (bvsle #x00000000 (bvadd #xfffffff7 A K))
     (bvsle #x00000000 (bvadd #xfffffff7 K G))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff I) G))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff C) G))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff B) D))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff K) D))
     (bvsle #x00000000 (bvadd #xfffffffe F (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffe F (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #xfffffffe F (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xfffffffe E (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffe E (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd #xfffffffe D (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffe A (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffe A (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd #xfffffff4 E G))
     (bvsle #x00000000 (bvadd #xfffffff4 E A))
     (bvsle #x00000000 (bvadd #xfffffff4 A G))
     (bvsle #x00000000 (bvadd #xfffffffc B H))
     (bvsle #x00000000 (bvadd #xfffffff6 I G))
     (bvsle #x00000000 (bvadd #xfffffff6 F D))
     (bvsle #x00000000 (bvadd #xfffffff6 E I))
     (bvsle #x00000000 (bvadd #xfffffff6 E C))
     (bvsle #x00000000 (bvadd #xfffffff6 C G))
     (bvsle #x00000000 (bvadd #xfffffff6 A I))
     (bvsle #x00000000 (bvadd #xfffffff6 A C))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff H) G))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff B) G))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff K) G))
     (bvsle #x00000000 (bvadd #xfffffffd E (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffd E (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #xfffffffd E (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xfffffffd A (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffd A (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #xfffffffd A (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff F) G))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff F) A))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff D) G))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff C) D))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff B) I))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff B) C))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff K) I))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff K) C))
     (bvsle #x00000000 (bvadd #xffffffff I (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xffffffff F (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xffffffff F (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd #xffffffff E (bvmul #xffffffff D)))
     (bvsle #x00000000 (bvadd #xffffffff D (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xffffffff C (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xffffffff A (bvmul #xffffffff D)))
     (bvsle #x00000000 (bvadd #xfffffff5 F G))
     (bvsle #x00000000 (bvadd #xfffffff5 F A))
     (bvsle #x00000000 (bvadd #xfffffff5 E F))
     (bvsle #x00000000 (bvadd #xfffffff5 E D))
     (bvsle #x00000000 (bvadd #xfffffff5 D G))
     (bvsle #x00000000 (bvadd #xfffffff5 A D))
     (bvsle #x00000000 (bvadd #xfffffff9 F H))
     (bvsle #x00000000 (bvadd #xfffffff9 F B))
     (bvsle #x00000000 (bvadd #xfffffff9 D H))
     (bvsle #x00000000 (bvadd #xfffffff9 B D))
     (bvsle #x00000000 (bvadd #xfffffff9 K I))
     (bvsle #x00000000 (bvadd #xfffffff9 K C))
     (bvsle #x00000000 (bvadd #x00000002 (bvmul #xffffffff E) I))
     (bvsle #x00000000 (bvadd #x00000002 (bvmul #xffffffff E) C))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff E) F))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff E) D))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff D) I))
     (bvsle #x00000000 (bvadd #x00000001 C (bvmul #xffffffff D)))
     (bvsle #x00000000 (bvadd #xfffffffb F))
     (bvsle #x00000000 (bvadd #xfffffffb D))
     (bvsle #x00000000 (bvadd #xfffffffa G))
     (bvsle #x00000000 (bvadd #xfffffffa E))
     (bvsle #x00000000 (bvadd #xfffffffa A))
     (bvsle #x00000000 (bvadd #xfffffffe H))
     (bvsle #x00000000 (bvadd #xfffffffe B))
     (bvsle #x00000000 (bvadd #xfffffffc I))
     (bvsle #x00000000 (bvadd #xfffffffc C))
     (bvsle #x00000000 (bvadd #xfffffffd K))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff E) G))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff E) A))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff C) I))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff B) H))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff A) G))
     (bvsle #x00000000 (bvadd F (bvmul #xffffffff D)))
     (bvsle #x00000000 (bvadd C (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd B (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd A (bvmul #xffffffff G)))
     (bvsle #x00000000 (bvadd #xfffffffb K H)))
      )
      (step_lturn A B C D K E F G H I J)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) ) 
    (=>
      (and
        (lturn A B C D K E F G H I J)
        true
      )
      (combined_lturn A B C D K E F G H I J)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn A B C D K E F G H I J)
        true
      )
      (combined_lturn A B C D K E F G H I J)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn A B C D H E F J I K G)
        true
      )
      (lturn A B C D H E F K J I G)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A B C D H E F I J L G)
        (step_lturn A B C D H E F I K J G)
        (combined_lturn A B C D H E F I L K G)
        true
      )
      (lturn A B C D H E F L K J G)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A B C D H E F I J L G)
        (combined_lturn A B C D H E F I K J G)
        (step_lturn A B C D H E F I L K G)
        true
      )
      (lturn A B C D H E F L K J G)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn A B C D H E F I J L G)
        (combined_lturn A B C D H E F I K J G)
        (combined_lturn A B C D H E F I L K G)
        true
      )
      (lturn A B C D H E F L K J G)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (v_12 (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A B C D H E F v_12 K I G)
        (step_lturn A B C D H E F L K J G)
        (combined_lturn A B C D H E F I K J G)
        (combined_lturn A B C D H E F I L K G)
        (combined_lturn A B C D H E F v_13 L K G)
        (and (= v_12 H) (= v_13 H) (= v_14 H))
      )
      (lturn A B C D H E F v_14 K J G)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (v_12 (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A B C D H E F v_12 K I G)
        (combined_lturn A B C D H E F L K J G)
        (step_lturn A B C D H E F I K J G)
        (combined_lturn A B C D H E F I L K G)
        (combined_lturn A B C D H E F v_13 L K G)
        (and (= v_12 H) (= v_13 H) (= v_14 H))
      )
      (lturn A B C D H E F v_14 K J G)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (v_12 (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A B C D H E F v_12 K I G)
        (combined_lturn A B C D H E F L K J G)
        (combined_lturn A B C D H E F I K J G)
        (step_lturn A B C D H E F I L K G)
        (combined_lturn A B C D H E F v_13 L K G)
        (and (= v_12 H) (= v_13 H) (= v_14 H))
      )
      (lturn A B C D H E F v_14 K J G)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (v_12 (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A B C D H E F v_12 K I G)
        (combined_lturn A B C D H E F L K J G)
        (combined_lturn A B C D H E F I K J G)
        (combined_lturn A B C D H E F I L K G)
        (step_lturn A B C D H E F v_13 L K G)
        (and (= v_12 H) (= v_13 H) (= v_14 H))
      )
      (lturn A B C D H E F v_14 K J G)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (v_12 (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn A B C D H E F v_12 K I G)
        (combined_lturn A B C D H E F L K J G)
        (combined_lturn A B C D H E F I K J G)
        (combined_lturn A B C D H E F I L K G)
        (combined_lturn A B C D H E F v_13 L K G)
        (and (= v_12 H) (= v_13 H) (= v_14 H))
      )
      (lturn A B C D H E F v_14 K J G)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn A B C D H E F J I K G)
        true
      )
      (step_lturn A B C D H E F K J I G)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A B C D H E F I J L G)
        (step_lturn A B C D H E F I K J G)
        (combined_lturn A B C D H E F I L K G)
        true
      )
      (step_lturn A B C D H E F L K J G)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A B C D H E F I J L G)
        (combined_lturn A B C D H E F I K J G)
        (step_lturn A B C D H E F I L K G)
        true
      )
      (step_lturn A B C D H E F L K J G)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn A B C D H E F I J L G)
        (combined_lturn A B C D H E F I K J G)
        (combined_lturn A B C D H E F I L K G)
        true
      )
      (step_lturn A B C D H E F L K J G)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (v_12 (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A B C D H E F v_12 K I G)
        (step_lturn A B C D H E F L K J G)
        (combined_lturn A B C D H E F I K J G)
        (combined_lturn A B C D H E F I L K G)
        (combined_lturn A B C D H E F v_13 L K G)
        (and (= v_12 H) (= v_13 H) (= v_14 H))
      )
      (step_lturn A B C D H E F v_14 K J G)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (v_12 (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A B C D H E F v_12 K I G)
        (combined_lturn A B C D H E F L K J G)
        (step_lturn A B C D H E F I K J G)
        (combined_lturn A B C D H E F I L K G)
        (combined_lturn A B C D H E F v_13 L K G)
        (and (= v_12 H) (= v_13 H) (= v_14 H))
      )
      (step_lturn A B C D H E F v_14 K J G)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (v_12 (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A B C D H E F v_12 K I G)
        (combined_lturn A B C D H E F L K J G)
        (combined_lturn A B C D H E F I K J G)
        (step_lturn A B C D H E F I L K G)
        (combined_lturn A B C D H E F v_13 L K G)
        (and (= v_12 H) (= v_13 H) (= v_14 H))
      )
      (step_lturn A B C D H E F v_14 K J G)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (v_12 (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A B C D H E F v_12 K I G)
        (combined_lturn A B C D H E F L K J G)
        (combined_lturn A B C D H E F I K J G)
        (combined_lturn A B C D H E F I L K G)
        (step_lturn A B C D H E F v_13 L K G)
        (and (= v_12 H) (= v_13 H) (= v_14 H))
      )
      (step_lturn A B C D H E F v_14 K J G)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (v_12 (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn A B C D H E F v_12 K I G)
        (combined_lturn A B C D H E F L K J G)
        (combined_lturn A B C D H E F I K J G)
        (combined_lturn A B C D H E F I L K G)
        (combined_lturn A B C D H E F v_13 L K G)
        (and (= v_12 H) (= v_13 H) (= v_14 H))
      )
      (step_lturn A B C D H E F v_14 K J G)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (v_12 (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A B C D H E F v_12 K I G)
        (step_lturn A B C D H E F v_13 J K G)
        (combined_lturn A B C D H E F L K J G)
        (combined_lturn A B C D H E F I K J G)
        (combined_lturn A B C D H E F I L K G)
        (combined_lturn A B C D H E F v_14 L K G)
        (and (= v_12 H) (= v_13 H) (= v_14 H))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (v_12 (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn A B C D H E F v_12 K I G)
        (combined_lturn A B C D H E F v_13 J K G)
        (combined_lturn A B C D H E F L K J G)
        (combined_lturn A B C D H E F I K J G)
        (combined_lturn A B C D H E F I L K G)
        (combined_lturn A B C D H E F v_14 L K G)
        (and (= v_12 H) (= v_13 H) (= v_14 H))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (v_12 (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A B C D H E F v_12 K I G)
        (combined_lturn A B C D H E F v_13 J K G)
        (combined_lturn A B C D H E F L K J G)
        (combined_lturn A B C D H E F I K J G)
        (combined_lturn A B C D H E F I L K G)
        (step_lturn A B C D H E F v_14 L K G)
        (and (= v_12 H) (= v_13 H) (= v_14 H))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (v_12 (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A B C D H E F v_12 K I G)
        (combined_lturn A B C D H E F v_13 J K G)
        (combined_lturn A B C D H E F L K J G)
        (combined_lturn A B C D H E F I K J G)
        (step_lturn A B C D H E F I L K G)
        (combined_lturn A B C D H E F v_14 L K G)
        (and (= v_12 H) (= v_13 H) (= v_14 H))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (v_12 (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A B C D H E F v_12 K I G)
        (combined_lturn A B C D H E F v_13 J K G)
        (combined_lturn A B C D H E F L K J G)
        (step_lturn A B C D H E F I K J G)
        (combined_lturn A B C D H E F I L K G)
        (combined_lturn A B C D H E F v_14 L K G)
        (and (= v_12 H) (= v_13 H) (= v_14 H))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (v_12 (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A B C D H E F v_12 K I G)
        (combined_lturn A B C D H E F v_13 J K G)
        (step_lturn A B C D H E F L K J G)
        (combined_lturn A B C D H E F I K J G)
        (combined_lturn A B C D H E F I L K G)
        (combined_lturn A B C D H E F v_14 L K G)
        (and (= v_12 H) (= v_13 H) (= v_14 H))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A B C D H E F I J L G)
        (step_lturn A B C D H E F L J K G)
        (combined_lturn A B C D H E F I K J G)
        (combined_lturn A B C D H E F I L K G)
        true
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn A B C D H E F I J L G)
        (combined_lturn A B C D H E F L J K G)
        (combined_lturn A B C D H E F I K J G)
        (combined_lturn A B C D H E F I L K G)
        true
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A B C D H E F I J L G)
        (combined_lturn A B C D H E F L J K G)
        (combined_lturn A B C D H E F I K J G)
        (step_lturn A B C D H E F I L K G)
        true
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A B C D H E F I J L G)
        (combined_lturn A B C D H E F L J K G)
        (step_lturn A B C D H E F I K J G)
        (combined_lturn A B C D H E F I L K G)
        true
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A B C D H E F K J I G)
        (step_lturn A B C D H E F K I J G)
        true
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn A B C D H E F K J I G)
        (combined_lturn A B C D H E F K I J G)
        true
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (CHC_COMP_UNUSED Bool) ) 
    (=>
      (and
        CHC_COMP_FALSE
      )
      false
    )
  )
)

(check-sat)
(exit)
