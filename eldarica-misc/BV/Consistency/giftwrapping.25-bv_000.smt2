(set-logic HORN)


(declare-fun |step_lturn| ( (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) ) Bool)
(declare-fun |combined_lturn| ( (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) ) Bool)
(declare-fun |CHC_COMP_FALSE| ( ) Bool)
(declare-fun |lturn| ( (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) ) Bool)

(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) ) 
    (=>
      (and
        (and (bvsle #x00000000 (bvadd #xfffffffc H K))
     (bvsle #x00000000 (bvadd #xfffffffc G J))
     (bvsle #x00000000 (bvadd #xfffffffc F H))
     (bvsle #x00000000 (bvadd #xfffffffc E H))
     (bvsle #x00000000 (bvadd #xfffffffc C I))
     (bvsle #x00000000 (bvadd #xfffffffc C J))
     (bvsle #x00000000 (bvadd #xfffffffc C G))
     (bvsle #x00000000 (bvadd #xfffffffc B H))
     (bvsle #x00000000 (bvadd #xfffffffc A H))
     (bvsle #x00000000 (bvadd #xfffffffc M H))
     (bvsle #x00000000 (bvadd #xfffffffa H J))
     (bvsle #x00000000 (bvadd #xfffffffa G H))
     (bvsle #x00000000 (bvadd #xfffffffa D I))
     (bvsle #x00000000 (bvadd #xfffffffa D J))
     (bvsle #x00000000 (bvadd #xfffffffa D G))
     (bvsle #x00000000 (bvadd #xfffffffa C D))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff A) D))
     (bvsle #x00000000 (bvadd #xfffffffd K I))
     (bvsle #x00000000 (bvadd #xfffffffd K J))
     (bvsle #x00000000 (bvadd #xfffffffd G K))
     (bvsle #x00000000 (bvadd #xfffffffd F I))
     (bvsle #x00000000 (bvadd #xfffffffd F J))
     (bvsle #x00000000 (bvadd #xfffffffd F G))
     (bvsle #x00000000 (bvadd #xfffffffd E I))
     (bvsle #x00000000 (bvadd #xfffffffd E J))
     (bvsle #x00000000 (bvadd #xfffffffd E G))
     (bvsle #x00000000 (bvadd #xfffffffd D (bvmul #xffffffff E)))
     (bvsle #x00000000 (bvadd #xfffffffd C K))
     (bvsle #x00000000 (bvadd #xfffffffd C F))
     (bvsle #x00000000 (bvadd #xfffffffd C E))
     (bvsle #x00000000 (bvadd #xfffffffd C B))
     (bvsle #x00000000 (bvadd #xfffffffd C M))
     (bvsle #x00000000 (bvadd #xfffffffd B I))
     (bvsle #x00000000 (bvadd #xfffffffd B J))
     (bvsle #x00000000 (bvadd #xfffffffd B G))
     (bvsle #x00000000 (bvadd #xfffffffd A I))
     (bvsle #x00000000 (bvadd #xfffffffd A J))
     (bvsle #x00000000 (bvadd #xfffffffd A G))
     (bvsle #x00000000 (bvadd #xfffffffd A C))
     (bvsle #x00000000 (bvadd #xfffffffd M I))
     (bvsle #x00000000 (bvadd #xfffffffd M J))
     (bvsle #x00000000 (bvadd #xfffffffd M G))
     (bvsle #x00000000 (bvadd #xfffffffb J I))
     (bvsle #x00000000 (bvadd #xfffffffb H I))
     (bvsle #x00000000 (bvadd #xfffffffb G I))
     (bvsle #x00000000 (bvadd #xfffffffb D K))
     (bvsle #x00000000 (bvadd #xfffffffb D F))
     (bvsle #x00000000 (bvadd #xfffffffb D E))
     (bvsle #x00000000 (bvadd #xfffffffb D B))
     (bvsle #x00000000 (bvadd #xfffffffb D M))
     (bvsle #x00000000 (bvadd #xfffffffb C H))
     (bvsle #x00000000 (bvadd #xfffffffb A D))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff E) H))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff C) D))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff A) H))
     (bvsle #x00000000 (bvadd #xfffffffe F K))
     (bvsle #x00000000 (bvadd #xfffffffe E K))
     (bvsle #x00000000 (bvadd #xfffffffe E F))
     (bvsle #x00000000 (bvadd #xfffffffe B K))
     (bvsle #x00000000 (bvadd #xfffffffe B F))
     (bvsle #x00000000 (bvadd #xfffffffe B E))
     (bvsle #x00000000 (bvadd #xfffffffe B M))
     (bvsle #x00000000 (bvadd #xfffffffe A K))
     (bvsle #x00000000 (bvadd #xfffffffe A F))
     (bvsle #x00000000 (bvadd #xfffffffe A E))
     (bvsle #x00000000 (bvadd #xfffffffe A B))
     (bvsle #x00000000 (bvadd #xfffffffe A M))
     (bvsle #x00000000 (bvadd #xfffffffe M K))
     (bvsle #x00000000 (bvadd #xfffffffe M F))
     (bvsle #x00000000 (bvadd #xfffffffe M E))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff E) I))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff E) J))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff E) G))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff C) H))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff B) J))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff B) G))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff A) I))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff A) J))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff A) G))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff A) C))
     (bvsle #x00000000 (bvadd #xffffffff H (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xffffffff D (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xffffffff D (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #xffffffff C (bvmul #xffffffff E)))
     (bvsle #x00000000
            (bvadd #x00000002 (bvmul #xffffffff A) (bvmul #xffffffff E)))
     (bvsle #x00000000
            (bvadd #x00000003 (bvmul #xffffffff C) (bvmul #xffffffff E)))
     (bvsle #x00000000
            (bvadd #x00000003 (bvmul #xffffffff A) (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff C) K))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff C) F))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff C) E))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff C) B))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff C) M))
     (bvsle #x00000000 (bvadd #x00000001 A (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd #xfffffffc D))
     (bvsle #x00000000 (bvadd #xfffffffd H))
     (bvsle #x00000000 (bvadd #xfffffffe I))
     (bvsle #x00000000 (bvadd #xfffffffe J))
     (bvsle #x00000000 (bvadd #xfffffffe G))
     (bvsle #x00000000 (bvadd #xfffffffe C))
     (bvsle #x00000000 (bvadd #xffffffff K))
     (bvsle #x00000000 (bvadd #xffffffff F))
     (bvsle #x00000000 (bvadd #xffffffff E))
     (bvsle #x00000000 (bvadd #xffffffff B))
     (bvsle #x00000000 (bvadd #xffffffff A))
     (bvsle #x00000000 (bvadd #xffffffff M))
     (bvsle #x00000000 (bvadd #x00000002 (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff G) J))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff F) K))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff E) K))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff E) F))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff C) I))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff C) J))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff C) G))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff B) K))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff B) F))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff B) M))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff A) K))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff A) F))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff A) E))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff A) B))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff A) M))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff M) K))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff M) F))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff E)))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff A)))
     (bvsle #x00000000 (bvadd G (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd F (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd D (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd B (bvmul #xffffffff E)))
     (bvsle #x00000000 (bvadd A (bvmul #xffffffff E)))
     (bvsle #x00000000 (bvadd M (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd M (bvmul #xffffffff F)))
     (bvsle #x00000000 (bvadd M (bvmul #xffffffff E)))
     (bvsle #x00000000 (bvadd #xfffffff9 D H)))
      )
      (lturn A M B C D E F G H I J K L)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) ) 
    (=>
      (and
        (and (bvsle #x00000000 (bvadd #xfffffffc H I))
     (bvsle #x00000000 (bvadd #xfffffffc H J))
     (bvsle #x00000000 (bvadd #xfffffffc G K))
     (bvsle #x00000000 (bvadd #xfffffffc F J))
     (bvsle #x00000000 (bvadd #xfffffffc E G))
     (bvsle #x00000000 (bvadd #xfffffffc C I))
     (bvsle #x00000000 (bvadd #xfffffffc C J))
     (bvsle #x00000000 (bvadd #xfffffffc C H))
     (bvsle #x00000000 (bvadd #xfffffffc C F))
     (bvsle #x00000000 (bvadd #xfffffffc C B))
     (bvsle #x00000000 (bvadd #xfffffffc C M))
     (bvsle #x00000000 (bvadd #xfffffffc B I))
     (bvsle #x00000000 (bvadd #xfffffffc B J))
     (bvsle #x00000000 (bvadd #xfffffffc B H))
     (bvsle #x00000000 (bvadd #xfffffffc B F))
     (bvsle #x00000000 (bvadd #xfffffffc B M))
     (bvsle #x00000000 (bvadd #xfffffffc A G))
     (bvsle #x00000000 (bvadd #xfffffffc M J))
     (bvsle #x00000000 (bvadd #xfffffffc M F))
     (bvsle #x00000000 (bvadd #xfffffffa D I))
     (bvsle #x00000000 (bvadd #xfffffffa D J))
     (bvsle #x00000000 (bvadd #xfffffffa D H))
     (bvsle #x00000000 (bvadd #xfffffffa D F))
     (bvsle #x00000000 (bvadd #xfffffffa D B))
     (bvsle #x00000000 (bvadd #xfffffffa D M))
     (bvsle #x00000000 (bvadd #xfffffffa C D))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff A) D))
     (bvsle #x00000000 (bvadd #xfffffffd K I))
     (bvsle #x00000000 (bvadd #xfffffffd K J))
     (bvsle #x00000000 (bvadd #xfffffffd H K))
     (bvsle #x00000000 (bvadd #xfffffffd F K))
     (bvsle #x00000000 (bvadd #xfffffffd E I))
     (bvsle #x00000000 (bvadd #xfffffffd E J))
     (bvsle #x00000000 (bvadd #xfffffffd E H))
     (bvsle #x00000000 (bvadd #xfffffffd E F))
     (bvsle #x00000000 (bvadd #xfffffffd D (bvmul #xffffffff E)))
     (bvsle #x00000000 (bvadd #xfffffffd C K))
     (bvsle #x00000000 (bvadd #xfffffffd C E))
     (bvsle #x00000000 (bvadd #xfffffffd B K))
     (bvsle #x00000000 (bvadd #xfffffffd B E))
     (bvsle #x00000000 (bvadd #xfffffffd A I))
     (bvsle #x00000000 (bvadd #xfffffffd A J))
     (bvsle #x00000000 (bvadd #xfffffffd A H))
     (bvsle #x00000000 (bvadd #xfffffffd A F))
     (bvsle #x00000000 (bvadd #xfffffffd A C))
     (bvsle #x00000000 (bvadd #xfffffffd A B))
     (bvsle #x00000000 (bvadd #xfffffffd A M))
     (bvsle #x00000000 (bvadd #xfffffffd M K))
     (bvsle #x00000000 (bvadd #xfffffffd M E))
     (bvsle #x00000000 (bvadd #xfffffffb J I))
     (bvsle #x00000000 (bvadd #xfffffffb G I))
     (bvsle #x00000000 (bvadd #xfffffffb G J))
     (bvsle #x00000000 (bvadd #xfffffffb G H))
     (bvsle #x00000000 (bvadd #xfffffffb F I))
     (bvsle #x00000000 (bvadd #xfffffffb F H))
     (bvsle #x00000000 (bvadd #xfffffffb F G))
     (bvsle #x00000000 (bvadd #xfffffffb D K))
     (bvsle #x00000000 (bvadd #xfffffffb D E))
     (bvsle #x00000000 (bvadd #xfffffffb C G))
     (bvsle #x00000000 (bvadd #xfffffffb B G))
     (bvsle #x00000000 (bvadd #xfffffffb A D))
     (bvsle #x00000000 (bvadd #xfffffffb M I))
     (bvsle #x00000000 (bvadd #xfffffffb M H))
     (bvsle #x00000000 (bvadd #xfffffffb M G))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff E) G))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff C) D))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff A) G))
     (bvsle #x00000000 (bvadd #xfffffffe E K))
     (bvsle #x00000000 (bvadd #xfffffffe A K))
     (bvsle #x00000000 (bvadd #xfffffffe A E))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff E) I))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff E) J))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff E) H))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff E) F))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff C) G))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff B) G))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff A) I))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff A) J))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff A) H))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff A) F))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff A) C))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff A) B))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff A) M))
     (bvsle #x00000000 (bvadd #xffffffff D (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #xffffffff C (bvmul #xffffffff E)))
     (bvsle #x00000000 (bvadd #xffffffff B (bvmul #xffffffff E)))
     (bvsle #x00000000 (bvadd #xffffffff M (bvmul #xffffffff E)))
     (bvsle #x00000000
            (bvadd #x00000002 (bvmul #xffffffff A) (bvmul #xffffffff E)))
     (bvsle #x00000000
            (bvadd #x00000003 (bvmul #xffffffff C) (bvmul #xffffffff E)))
     (bvsle #x00000000
            (bvadd #x00000003 (bvmul #xffffffff A) (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff C) K))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff C) E))
     (bvsle #x00000000 (bvadd #x00000001 F (bvmul #xffffffff G)))
     (bvsle #x00000000 (bvadd #x00000001 A (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd #x00000001 M (bvmul #xffffffff G)))
     (bvsle #x00000000 (bvadd #xfffffffc D))
     (bvsle #x00000000 (bvadd #xfffffffd G))
     (bvsle #x00000000 (bvadd #xfffffffe I))
     (bvsle #x00000000 (bvadd #xfffffffe J))
     (bvsle #x00000000 (bvadd #xfffffffe H))
     (bvsle #x00000000 (bvadd #xfffffffe F))
     (bvsle #x00000000 (bvadd #xfffffffe C))
     (bvsle #x00000000 (bvadd #xfffffffe B))
     (bvsle #x00000000 (bvadd #xfffffffe M))
     (bvsle #x00000000 (bvadd #xffffffff K))
     (bvsle #x00000000 (bvadd #xffffffff E))
     (bvsle #x00000000 (bvadd #xffffffff A))
     (bvsle #x00000000 (bvadd #x00000002 (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff E) K))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff C) I))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff C) J))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff C) H))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff C) F))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff C) B))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff C) M))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff B) F))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff B) M))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff A) K))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff A) E))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff M) F))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff E)))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff A)))
     (bvsle #x00000000 (bvadd D (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd D (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd D (bvmul #xffffffff G)))
     (bvsle #x00000000 (bvadd A (bvmul #xffffffff E)))
     (bvsle #x00000000 (bvadd M (bvmul #xffffffff F)))
     (bvsle #x00000000 (bvadd #xfffffff9 D G)))
      )
      (lturn A M B C D E F G H I J K L)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) ) 
    (=>
      (and
        (and (bvsle #x00000000 (bvadd #xfffffffc G J))
     (bvsle #x00000000 (bvadd #xfffffffc C I))
     (bvsle #x00000000 (bvadd #xfffffffc C J))
     (bvsle #x00000000 (bvadd #xfffffffc C H))
     (bvsle #x00000000 (bvadd #xfffffffc C G))
     (bvsle #x00000000 (bvadd #xfffffffa D I))
     (bvsle #x00000000 (bvadd #xfffffffa D J))
     (bvsle #x00000000 (bvadd #xfffffffa D H))
     (bvsle #x00000000 (bvadd #xfffffffa D G))
     (bvsle #x00000000 (bvadd #xfffffffa C D))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff A) D))
     (bvsle #x00000000 (bvadd #xfffffffd K I))
     (bvsle #x00000000 (bvadd #xfffffffd K J))
     (bvsle #x00000000 (bvadd #xfffffffd H K))
     (bvsle #x00000000 (bvadd #xfffffffd G K))
     (bvsle #x00000000 (bvadd #xfffffffd F I))
     (bvsle #x00000000 (bvadd #xfffffffd F J))
     (bvsle #x00000000 (bvadd #xfffffffd F H))
     (bvsle #x00000000 (bvadd #xfffffffd F G))
     (bvsle #x00000000 (bvadd #xfffffffd E I))
     (bvsle #x00000000 (bvadd #xfffffffd E J))
     (bvsle #x00000000 (bvadd #xfffffffd E H))
     (bvsle #x00000000 (bvadd #xfffffffd E G))
     (bvsle #x00000000 (bvadd #xfffffffd D (bvmul #xffffffff E)))
     (bvsle #x00000000 (bvadd #xfffffffd C K))
     (bvsle #x00000000 (bvadd #xfffffffd C F))
     (bvsle #x00000000 (bvadd #xfffffffd C E))
     (bvsle #x00000000 (bvadd #xfffffffd C B))
     (bvsle #x00000000 (bvadd #xfffffffd C M))
     (bvsle #x00000000 (bvadd #xfffffffd B I))
     (bvsle #x00000000 (bvadd #xfffffffd B J))
     (bvsle #x00000000 (bvadd #xfffffffd B H))
     (bvsle #x00000000 (bvadd #xfffffffd B G))
     (bvsle #x00000000 (bvadd #xfffffffd A I))
     (bvsle #x00000000 (bvadd #xfffffffd A J))
     (bvsle #x00000000 (bvadd #xfffffffd A H))
     (bvsle #x00000000 (bvadd #xfffffffd A G))
     (bvsle #x00000000 (bvadd #xfffffffd A C))
     (bvsle #x00000000 (bvadd #xfffffffd M I))
     (bvsle #x00000000 (bvadd #xfffffffd M J))
     (bvsle #x00000000 (bvadd #xfffffffd M H))
     (bvsle #x00000000 (bvadd #xfffffffd M G))
     (bvsle #x00000000 (bvadd #xfffffffb J I))
     (bvsle #x00000000 (bvadd #xfffffffb H J))
     (bvsle #x00000000 (bvadd #xfffffffb G I))
     (bvsle #x00000000 (bvadd #xfffffffb G H))
     (bvsle #x00000000 (bvadd #xfffffffb D K))
     (bvsle #x00000000 (bvadd #xfffffffb D F))
     (bvsle #x00000000 (bvadd #xfffffffb D E))
     (bvsle #x00000000 (bvadd #xfffffffb D B))
     (bvsle #x00000000 (bvadd #xfffffffb D M))
     (bvsle #x00000000 (bvadd #xfffffffb A D))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff C) D))
     (bvsle #x00000000 (bvadd #xfffffffe F K))
     (bvsle #x00000000 (bvadd #xfffffffe E K))
     (bvsle #x00000000 (bvadd #xfffffffe E F))
     (bvsle #x00000000 (bvadd #xfffffffe B K))
     (bvsle #x00000000 (bvadd #xfffffffe B F))
     (bvsle #x00000000 (bvadd #xfffffffe B E))
     (bvsle #x00000000 (bvadd #xfffffffe B M))
     (bvsle #x00000000 (bvadd #xfffffffe A K))
     (bvsle #x00000000 (bvadd #xfffffffe A F))
     (bvsle #x00000000 (bvadd #xfffffffe A E))
     (bvsle #x00000000 (bvadd #xfffffffe A B))
     (bvsle #x00000000 (bvadd #xfffffffe A M))
     (bvsle #x00000000 (bvadd #xfffffffe M K))
     (bvsle #x00000000 (bvadd #xfffffffe M F))
     (bvsle #x00000000 (bvadd #xfffffffe M E))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff E) I))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff E) J))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff E) H))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff E) G))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff B) J))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff B) G))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff A) I))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff A) J))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff A) H))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff A) G))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff A) C))
     (bvsle #x00000000 (bvadd #xffffffff D (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #xffffffff C (bvmul #xffffffff E)))
     (bvsle #x00000000
            (bvadd #x00000002 (bvmul #xffffffff A) (bvmul #xffffffff E)))
     (bvsle #x00000000
            (bvadd #x00000003 (bvmul #xffffffff C) (bvmul #xffffffff E)))
     (bvsle #x00000000
            (bvadd #x00000003 (bvmul #xffffffff A) (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff C) K))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff C) F))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff C) E))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff C) B))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff C) M))
     (bvsle #x00000000 (bvadd #x00000001 A (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd #xfffffffc D))
     (bvsle #x00000000 (bvadd #xfffffffe I))
     (bvsle #x00000000 (bvadd #xfffffffe J))
     (bvsle #x00000000 (bvadd #xfffffffe H))
     (bvsle #x00000000 (bvadd #xfffffffe G))
     (bvsle #x00000000 (bvadd #xfffffffe C))
     (bvsle #x00000000 (bvadd #xffffffff K))
     (bvsle #x00000000 (bvadd #xffffffff F))
     (bvsle #x00000000 (bvadd #xffffffff E))
     (bvsle #x00000000 (bvadd #xffffffff B))
     (bvsle #x00000000 (bvadd #xffffffff A))
     (bvsle #x00000000 (bvadd #xffffffff M))
     (bvsle #x00000000 (bvadd #x00000002 (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff H) I))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff G) J))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff F) K))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff E) K))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff E) F))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff C) I))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff C) J))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff C) H))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff C) G))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff B) K))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff B) F))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff B) M))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff A) K))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff A) F))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff A) E))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff A) B))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff A) M))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff M) K))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff M) F))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff E)))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff A)))
     (bvsle #x00000000 (bvadd H (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd G (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd F (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd D (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd D (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd B (bvmul #xffffffff E)))
     (bvsle #x00000000 (bvadd A (bvmul #xffffffff E)))
     (bvsle #x00000000 (bvadd M (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd M (bvmul #xffffffff F)))
     (bvsle #x00000000 (bvadd M (bvmul #xffffffff E)))
     (bvsle #x00000000 (bvadd #xfffffffc H I)))
      )
      (step_lturn A M B C D E F G H I J K L)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) ) 
    (=>
      (and
        (lturn A M B C D E F G H I J K L)
        true
      )
      (combined_lturn A M B C D E F G H I J K L)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn A M B C D E F G H I J K L)
        true
      )
      (combined_lturn A M B C D E F G H I J K L)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn A J B C D E F G H L K M I)
        true
      )
      (lturn A J B C D E F G H M L K I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A J B C D E F G H v_13 K M I)
        (step_lturn A J B C D E F G H v_14 L K I)
        (combined_lturn A J B C D E F G H v_15 M L I)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      (lturn A J B C D E F G H M L K I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A J B C D E F G H v_13 K M I)
        (combined_lturn A J B C D E F G H v_14 L K I)
        (step_lturn A J B C D E F G H v_15 M L I)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      (lturn A J B C D E F G H M L K I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn A J B C D E F G H v_13 K M I)
        (combined_lturn A J B C D E F G H v_14 L K I)
        (combined_lturn A J B C D E F G H v_15 M L I)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      (lturn A J B C D E F G H M L K I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (N (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) (v_16 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A K B C D E F G H J M v_14 I)
        (step_lturn A K B C D E F G H N M L I)
        (combined_lturn A K B C D E F G H v_15 M L I)
        (combined_lturn A K B C D E F G H v_16 N M I)
        (combined_lturn A K B C D E F G H J N M I)
        (and (= v_14 K) (= v_15 K) (= v_16 K))
      )
      (lturn A K B C D E F G H J M L I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (N (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) (v_16 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A K B C D E F G H J M v_14 I)
        (combined_lturn A K B C D E F G H N M L I)
        (step_lturn A K B C D E F G H v_15 M L I)
        (combined_lturn A K B C D E F G H v_16 N M I)
        (combined_lturn A K B C D E F G H J N M I)
        (and (= v_14 K) (= v_15 K) (= v_16 K))
      )
      (lturn A K B C D E F G H J M L I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (N (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) (v_16 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A K B C D E F G H J M v_14 I)
        (combined_lturn A K B C D E F G H N M L I)
        (combined_lturn A K B C D E F G H v_15 M L I)
        (step_lturn A K B C D E F G H v_16 N M I)
        (combined_lturn A K B C D E F G H J N M I)
        (and (= v_14 K) (= v_15 K) (= v_16 K))
      )
      (lturn A K B C D E F G H J M L I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (N (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) (v_16 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A K B C D E F G H J M v_14 I)
        (combined_lturn A K B C D E F G H N M L I)
        (combined_lturn A K B C D E F G H v_15 M L I)
        (combined_lturn A K B C D E F G H v_16 N M I)
        (step_lturn A K B C D E F G H J N M I)
        (and (= v_14 K) (= v_15 K) (= v_16 K))
      )
      (lturn A K B C D E F G H J M L I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (N (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) (v_16 (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn A K B C D E F G H J M v_14 I)
        (combined_lturn A K B C D E F G H N M L I)
        (combined_lturn A K B C D E F G H v_15 M L I)
        (combined_lturn A K B C D E F G H v_16 N M I)
        (combined_lturn A K B C D E F G H J N M I)
        (and (= v_14 K) (= v_15 K) (= v_16 K))
      )
      (lturn A K B C D E F G H J M L I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn A J B C D E F G H L K M I)
        true
      )
      (step_lturn A J B C D E F G H M L K I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A J B C D E F G H v_13 K M I)
        (step_lturn A J B C D E F G H v_14 L K I)
        (combined_lturn A J B C D E F G H v_15 M L I)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      (step_lturn A J B C D E F G H M L K I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A J B C D E F G H v_13 K M I)
        (combined_lturn A J B C D E F G H v_14 L K I)
        (step_lturn A J B C D E F G H v_15 M L I)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      (step_lturn A J B C D E F G H M L K I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn A J B C D E F G H v_13 K M I)
        (combined_lturn A J B C D E F G H v_14 L K I)
        (combined_lturn A J B C D E F G H v_15 M L I)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      (step_lturn A J B C D E F G H M L K I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (N (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) (v_16 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A K B C D E F G H J M v_14 I)
        (step_lturn A K B C D E F G H N M L I)
        (combined_lturn A K B C D E F G H v_15 M L I)
        (combined_lturn A K B C D E F G H v_16 N M I)
        (combined_lturn A K B C D E F G H J N M I)
        (and (= v_14 K) (= v_15 K) (= v_16 K))
      )
      (step_lturn A K B C D E F G H J M L I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (N (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) (v_16 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A K B C D E F G H J M v_14 I)
        (combined_lturn A K B C D E F G H N M L I)
        (step_lturn A K B C D E F G H v_15 M L I)
        (combined_lturn A K B C D E F G H v_16 N M I)
        (combined_lturn A K B C D E F G H J N M I)
        (and (= v_14 K) (= v_15 K) (= v_16 K))
      )
      (step_lturn A K B C D E F G H J M L I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (N (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) (v_16 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A K B C D E F G H J M v_14 I)
        (combined_lturn A K B C D E F G H N M L I)
        (combined_lturn A K B C D E F G H v_15 M L I)
        (step_lturn A K B C D E F G H v_16 N M I)
        (combined_lturn A K B C D E F G H J N M I)
        (and (= v_14 K) (= v_15 K) (= v_16 K))
      )
      (step_lturn A K B C D E F G H J M L I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (N (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) (v_16 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A K B C D E F G H J M v_14 I)
        (combined_lturn A K B C D E F G H N M L I)
        (combined_lturn A K B C D E F G H v_15 M L I)
        (combined_lturn A K B C D E F G H v_16 N M I)
        (step_lturn A K B C D E F G H J N M I)
        (and (= v_14 K) (= v_15 K) (= v_16 K))
      )
      (step_lturn A K B C D E F G H J M L I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (N (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) (v_16 (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn A K B C D E F G H J M v_14 I)
        (combined_lturn A K B C D E F G H N M L I)
        (combined_lturn A K B C D E F G H v_15 M L I)
        (combined_lturn A K B C D E F G H v_16 N M I)
        (combined_lturn A K B C D E F G H J N M I)
        (and (= v_14 K) (= v_15 K) (= v_16 K))
      )
      (step_lturn A K B C D E F G H J M L I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (N (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) (v_16 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A K B C D E F G H J M v_14 I)
        (step_lturn A K B C D E F G H J L M I)
        (combined_lturn A K B C D E F G H N M L I)
        (combined_lturn A K B C D E F G H v_15 M L I)
        (combined_lturn A K B C D E F G H v_16 N M I)
        (combined_lturn A K B C D E F G H J N M I)
        (and (= v_14 K) (= v_15 K) (= v_16 K))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (N (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) (v_16 (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn A K B C D E F G H J M v_14 I)
        (combined_lturn A K B C D E F G H J L M I)
        (combined_lturn A K B C D E F G H N M L I)
        (combined_lturn A K B C D E F G H v_15 M L I)
        (combined_lturn A K B C D E F G H v_16 N M I)
        (combined_lturn A K B C D E F G H J N M I)
        (and (= v_14 K) (= v_15 K) (= v_16 K))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (N (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) (v_16 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A K B C D E F G H J M v_14 I)
        (combined_lturn A K B C D E F G H J L M I)
        (combined_lturn A K B C D E F G H N M L I)
        (combined_lturn A K B C D E F G H v_15 M L I)
        (combined_lturn A K B C D E F G H v_16 N M I)
        (step_lturn A K B C D E F G H J N M I)
        (and (= v_14 K) (= v_15 K) (= v_16 K))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (N (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) (v_16 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A K B C D E F G H J M v_14 I)
        (combined_lturn A K B C D E F G H J L M I)
        (combined_lturn A K B C D E F G H N M L I)
        (combined_lturn A K B C D E F G H v_15 M L I)
        (step_lturn A K B C D E F G H v_16 N M I)
        (combined_lturn A K B C D E F G H J N M I)
        (and (= v_14 K) (= v_15 K) (= v_16 K))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (N (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) (v_16 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A K B C D E F G H J M v_14 I)
        (combined_lturn A K B C D E F G H J L M I)
        (combined_lturn A K B C D E F G H N M L I)
        (step_lturn A K B C D E F G H v_15 M L I)
        (combined_lturn A K B C D E F G H v_16 N M I)
        (combined_lturn A K B C D E F G H J N M I)
        (and (= v_14 K) (= v_15 K) (= v_16 K))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (N (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) (v_16 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A K B C D E F G H J M v_14 I)
        (combined_lturn A K B C D E F G H J L M I)
        (step_lturn A K B C D E F G H N M L I)
        (combined_lturn A K B C D E F G H v_15 M L I)
        (combined_lturn A K B C D E F G H v_16 N M I)
        (combined_lturn A K B C D E F G H J N M I)
        (and (= v_14 K) (= v_15 K) (= v_16 K))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A J B C D E F G H v_13 K M I)
        (step_lturn A J B C D E F G H M K L I)
        (combined_lturn A J B C D E F G H v_14 L K I)
        (combined_lturn A J B C D E F G H v_15 M L I)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn A J B C D E F G H v_13 K M I)
        (combined_lturn A J B C D E F G H M K L I)
        (combined_lturn A J B C D E F G H v_14 L K I)
        (combined_lturn A J B C D E F G H v_15 M L I)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A J B C D E F G H v_13 K M I)
        (combined_lturn A J B C D E F G H M K L I)
        (combined_lturn A J B C D E F G H v_14 L K I)
        (step_lturn A J B C D E F G H v_15 M L I)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A J B C D E F G H v_13 K M I)
        (combined_lturn A J B C D E F G H M K L I)
        (step_lturn A J B C D E F G H v_14 L K I)
        (combined_lturn A J B C D E F G H v_15 M L I)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A J B C D E F G H M L K I)
        (step_lturn A J B C D E F G H M K L I)
        true
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn A J B C D E F G H M L K I)
        (combined_lturn A J B C D E F G H M K L I)
        true
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (CHC_COMP_UNUSED Bool) ) 
    (=>
      (and
        CHC_COMP_FALSE
      )
      false
    )
  )
)

(check-sat)
(exit)
