(set-logic HORN)


(declare-fun |step_lturn| ( (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) ) Bool)
(declare-fun |combined_lturn| ( (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) ) Bool)
(declare-fun |CHC_COMP_FALSE| ( ) Bool)
(declare-fun |lturn| ( (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) ) Bool)

(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) ) 
    (=>
      (and
        (and (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff M) I))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff G) B))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff G) I))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff C) F))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff C) E))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff C) D))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff C) B))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff C) I))
     (bvsle #x00000000 (bvadd #xfffffffd F (bvmul #xffffffff M)))
     (bvsle #x00000000 (bvadd #xfffffffd F (bvmul #xffffffff G)))
     (bvsle #x00000000 (bvadd #xfffffffd E (bvmul #xffffffff M)))
     (bvsle #x00000000 (bvadd #xfffffffd E (bvmul #xffffffff G)))
     (bvsle #x00000000 (bvadd #xfffffffd D (bvmul #xffffffff M)))
     (bvsle #x00000000 (bvadd #xfffffffd D (bvmul #xffffffff G)))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff M) J))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff K) I))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff H) B))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff H) I))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff G) J))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff C) J))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff A) B))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff A) I))
     (bvsle #x00000000 (bvadd #xfffffffe F (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xfffffffe F (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffe F (bvmul #xffffffff A)))
     (bvsle #x00000000 (bvadd #xfffffffe E (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xfffffffe E (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffe E (bvmul #xffffffff A)))
     (bvsle #x00000000 (bvadd #xfffffffe D (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xfffffffe D (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffe D (bvmul #xffffffff A)))
     (bvsle #x00000000 (bvadd #xfffffffe B (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff M) A))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff K) J))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff J) I))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff H) J))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff G) H))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff G) A))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff C) H))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff C) A))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff A) J))
     (bvsle #x00000000 (bvadd #xffffffff H (bvmul #xffffffff M)))
     (bvsle #x00000000 (bvadd #xffffffff F (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #xffffffff E (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #xffffffff D (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #xffffffff B (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff H) M))
     (bvsle #x00000000 (bvadd #x00000001 M (bvmul #xffffffff A)))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff H) A))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff G) M))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff G) K))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff E) B))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff E) I))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff D) F))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff C) M))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff C) K))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff C) G))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff B) I))
     (bvsle #x00000000 (bvadd H (bvmul #xffffffff A)))
     (bvsle #x00000000 (bvadd E (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd E (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd D (bvmul #xffffffff F)))
     (bvsle #x00000000 (bvadd C (bvmul #xffffffff G)))
     (bvsle #x00000000 (bvadd B (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff M) B)))
      )
      (lturn M A B C D E F G H I J K L)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) ) 
    (=>
      (and
        (and (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff G) B))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff C) F))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff C) E))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff C) D))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff C) B))
     (bvsle #x00000000 (bvadd #xfffffffd F (bvmul #xffffffff M)))
     (bvsle #x00000000 (bvadd #xfffffffd F (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xfffffffd F (bvmul #xffffffff G)))
     (bvsle #x00000000 (bvadd #xfffffffd E (bvmul #xffffffff M)))
     (bvsle #x00000000 (bvadd #xfffffffd E (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xfffffffd E (bvmul #xffffffff G)))
     (bvsle #x00000000 (bvadd #xfffffffd D (bvmul #xffffffff M)))
     (bvsle #x00000000 (bvadd #xfffffffd D (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xfffffffd D (bvmul #xffffffff G)))
     (bvsle #x00000000 (bvadd #xfffffffd B (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff M) A))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff M) I))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff K) I))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff G) H))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff G) A))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff G) I))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff C) H))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff C) A))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff C) I))
     (bvsle #x00000000 (bvadd #xfffffffe H (bvmul #xffffffff M)))
     (bvsle #x00000000 (bvadd #xfffffffe H (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xfffffffe F (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #xfffffffe E (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #xfffffffe D (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #xfffffffe B (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #xfffffffe A (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff M) J))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff K) J))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff J) I))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff H) B))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff G) J))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff C) J))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff A) B))
     (bvsle #x00000000 (bvadd #xffffffff H (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #xffffffff F (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xffffffff F (bvmul #xffffffff A)))
     (bvsle #x00000000 (bvadd #xffffffff F (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xffffffff E (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xffffffff E (bvmul #xffffffff A)))
     (bvsle #x00000000 (bvadd #xffffffff E (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xffffffff D (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xffffffff D (bvmul #xffffffff A)))
     (bvsle #x00000000 (bvadd #xffffffff D (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xffffffff B (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xffffffff A (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff E) H))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff E) A))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff E) I))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff B) I))
     (bvsle #x00000000 (bvadd #x00000001 M (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #x00000001 K (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #x00000001 H (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #x00000001 G (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #x00000001 C (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #x00000001 A (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff M) K))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff H) A))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff H) I))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff G) M))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff G) K))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff E) F))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff E) D))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff E) B))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff D) F))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff C) M))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff C) K))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff C) G))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff A) I))
     (bvsle #x00000000 (bvadd M (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd H (bvmul #xffffffff A)))
     (bvsle #x00000000 (bvadd H (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd G (bvmul #xffffffff M)))
     (bvsle #x00000000 (bvadd G (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd F (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd E (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd D (bvmul #xffffffff F)))
     (bvsle #x00000000 (bvadd D (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd C (bvmul #xffffffff M)))
     (bvsle #x00000000 (bvadd C (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd C (bvmul #xffffffff G)))
     (bvsle #x00000000 (bvadd A (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff M) B)))
      )
      (lturn M A B C D E F G H I J K L)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) ) 
    (=>
      (and
        (and (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff C) F))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff C) E))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff C) D))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff C) B))
     (bvsle #x00000000 (bvadd #xfffffffd F (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xfffffffd F (bvmul #xffffffff G)))
     (bvsle #x00000000 (bvadd #xfffffffd E (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xfffffffd E (bvmul #xffffffff G)))
     (bvsle #x00000000 (bvadd #xfffffffd D (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xfffffffd D (bvmul #xffffffff G)))
     (bvsle #x00000000 (bvadd #xfffffffd B (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff M) B))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff K) I))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff G) I))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff C) I))
     (bvsle #x00000000 (bvadd #xfffffffe F (bvmul #xffffffff M)))
     (bvsle #x00000000 (bvadd #xfffffffe F (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #xfffffffe E (bvmul #xffffffff M)))
     (bvsle #x00000000 (bvadd #xfffffffe E (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #xfffffffe D (bvmul #xffffffff M)))
     (bvsle #x00000000 (bvadd #xfffffffe D (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #xfffffffe B (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff M) A))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff K) J))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff J) I))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff H) B))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff G) J))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff G) H))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff G) A))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff C) J))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff C) H))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff C) A))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff A) B))
     (bvsle #x00000000 (bvadd #xffffffff H (bvmul #xffffffff M)))
     (bvsle #x00000000 (bvadd #xffffffff F (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xffffffff F (bvmul #xffffffff A)))
     (bvsle #x00000000 (bvadd #xffffffff F (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xffffffff E (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xffffffff E (bvmul #xffffffff A)))
     (bvsle #x00000000 (bvadd #xffffffff E (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xffffffff D (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xffffffff D (bvmul #xffffffff A)))
     (bvsle #x00000000 (bvadd #xffffffff D (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xffffffff B (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff H) A))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff G) M))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff G) K))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff E) F))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff E) D))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff E) B))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff D) F))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff C) M))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff C) K))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff C) G))
     (bvsle #x00000000 (bvadd H (bvmul #xffffffff A)))
     (bvsle #x00000000 (bvadd F (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd E (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd D (bvmul #xffffffff F)))
     (bvsle #x00000000 (bvadd D (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd C (bvmul #xffffffff G)))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff G) B)))
      )
      (lturn M A B C D E F G H I J K L)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) ) 
    (=>
      (and
        (and (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff C) D))
     (bvsle #x00000000 (bvadd #xfffffffd F (bvmul #xffffffff G)))
     (bvsle #x00000000 (bvadd #xfffffffd D (bvmul #xffffffff G)))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff M) B))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff M) I))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff K) I))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff G) B))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff G) I))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff C) E))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff C) B))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff C) I))
     (bvsle #x00000000 (bvadd #xfffffffe F (bvmul #xffffffff M)))
     (bvsle #x00000000 (bvadd #xfffffffe F (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xfffffffe E (bvmul #xffffffff M)))
     (bvsle #x00000000 (bvadd #xfffffffe E (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xfffffffe E (bvmul #xffffffff G)))
     (bvsle #x00000000 (bvadd #xfffffffe D (bvmul #xffffffff M)))
     (bvsle #x00000000 (bvadd #xfffffffe D (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xfffffffe B (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff M) J))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff M) A))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff K) J))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff J) I))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff H) B))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff H) I))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff G) J))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff G) H))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff G) A))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff C) J))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff C) H))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff C) A))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff A) B))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff A) I))
     (bvsle #x00000000 (bvadd #xffffffff H (bvmul #xffffffff M)))
     (bvsle #x00000000 (bvadd #xffffffff H (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd #xffffffff F (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #xffffffff F (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xffffffff F (bvmul #xffffffff A)))
     (bvsle #x00000000 (bvadd #xffffffff E (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #xffffffff E (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xffffffff E (bvmul #xffffffff A)))
     (bvsle #x00000000 (bvadd #xffffffff D (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #xffffffff D (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xffffffff D (bvmul #xffffffff A)))
     (bvsle #x00000000 (bvadd #xffffffff B (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #xffffffff A (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff M) K))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff H) J))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff H) A))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff G) M))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff G) K))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff E) B))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff E) I))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff D) F))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff C) M))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff C) K))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff C) G))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff B) I))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff A) J))
     (bvsle #x00000000 (bvadd M (bvmul #xffffffff K)))
     (bvsle #x00000000 (bvadd H (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd H (bvmul #xffffffff A)))
     (bvsle #x00000000 (bvadd E (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd E (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd D (bvmul #xffffffff F)))
     (bvsle #x00000000 (bvadd C (bvmul #xffffffff G)))
     (bvsle #x00000000 (bvadd B (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd A (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff C) F)))
      )
      (step_lturn M A B C D E F G H I J K L)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) ) 
    (=>
      (and
        (lturn M A B C D E F G H I J K L)
        true
      )
      (combined_lturn M A B C D E F G H I J K L)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn M A B C D E F G H I J K L)
        true
      )
      (combined_lturn M A B C D E F G H I J K L)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn J A B C D E F G H L K M I)
        true
      )
      (lturn J A B C D E F G H M L K I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn J A B C D E F G H v_13 K M I)
        (step_lturn J A B C D E F G H v_14 L K I)
        (combined_lturn J A B C D E F G H v_15 M L I)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      (lturn J A B C D E F G H M L K I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn J A B C D E F G H v_13 K M I)
        (combined_lturn J A B C D E F G H v_14 L K I)
        (step_lturn J A B C D E F G H v_15 M L I)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      (lturn J A B C D E F G H M L K I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn J A B C D E F G H v_13 K M I)
        (combined_lturn J A B C D E F G H v_14 L K I)
        (combined_lturn J A B C D E F G H v_15 M L I)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      (lturn J A B C D E F G H M L K I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (N (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) (v_16 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn K A B C D E F G H J M v_14 I)
        (step_lturn K A B C D E F G H N M L I)
        (combined_lturn K A B C D E F G H v_15 M L I)
        (combined_lturn K A B C D E F G H v_16 N M I)
        (combined_lturn K A B C D E F G H J N M I)
        (and (= v_14 K) (= v_15 K) (= v_16 K))
      )
      (lturn K A B C D E F G H J M L I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (N (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) (v_16 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn K A B C D E F G H J M v_14 I)
        (combined_lturn K A B C D E F G H N M L I)
        (step_lturn K A B C D E F G H v_15 M L I)
        (combined_lturn K A B C D E F G H v_16 N M I)
        (combined_lturn K A B C D E F G H J N M I)
        (and (= v_14 K) (= v_15 K) (= v_16 K))
      )
      (lturn K A B C D E F G H J M L I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (N (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) (v_16 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn K A B C D E F G H J M v_14 I)
        (combined_lturn K A B C D E F G H N M L I)
        (combined_lturn K A B C D E F G H v_15 M L I)
        (step_lturn K A B C D E F G H v_16 N M I)
        (combined_lturn K A B C D E F G H J N M I)
        (and (= v_14 K) (= v_15 K) (= v_16 K))
      )
      (lturn K A B C D E F G H J M L I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (N (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) (v_16 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn K A B C D E F G H J M v_14 I)
        (combined_lturn K A B C D E F G H N M L I)
        (combined_lturn K A B C D E F G H v_15 M L I)
        (combined_lturn K A B C D E F G H v_16 N M I)
        (step_lturn K A B C D E F G H J N M I)
        (and (= v_14 K) (= v_15 K) (= v_16 K))
      )
      (lturn K A B C D E F G H J M L I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (N (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) (v_16 (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn K A B C D E F G H J M v_14 I)
        (combined_lturn K A B C D E F G H N M L I)
        (combined_lturn K A B C D E F G H v_15 M L I)
        (combined_lturn K A B C D E F G H v_16 N M I)
        (combined_lturn K A B C D E F G H J N M I)
        (and (= v_14 K) (= v_15 K) (= v_16 K))
      )
      (lturn K A B C D E F G H J M L I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn J A B C D E F G H L K M I)
        true
      )
      (step_lturn J A B C D E F G H M L K I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn J A B C D E F G H v_13 K M I)
        (step_lturn J A B C D E F G H v_14 L K I)
        (combined_lturn J A B C D E F G H v_15 M L I)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      (step_lturn J A B C D E F G H M L K I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn J A B C D E F G H v_13 K M I)
        (combined_lturn J A B C D E F G H v_14 L K I)
        (step_lturn J A B C D E F G H v_15 M L I)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      (step_lturn J A B C D E F G H M L K I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn J A B C D E F G H v_13 K M I)
        (combined_lturn J A B C D E F G H v_14 L K I)
        (combined_lturn J A B C D E F G H v_15 M L I)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      (step_lturn J A B C D E F G H M L K I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (N (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) (v_16 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn K A B C D E F G H J M v_14 I)
        (step_lturn K A B C D E F G H N M L I)
        (combined_lturn K A B C D E F G H v_15 M L I)
        (combined_lturn K A B C D E F G H v_16 N M I)
        (combined_lturn K A B C D E F G H J N M I)
        (and (= v_14 K) (= v_15 K) (= v_16 K))
      )
      (step_lturn K A B C D E F G H J M L I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (N (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) (v_16 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn K A B C D E F G H J M v_14 I)
        (combined_lturn K A B C D E F G H N M L I)
        (step_lturn K A B C D E F G H v_15 M L I)
        (combined_lturn K A B C D E F G H v_16 N M I)
        (combined_lturn K A B C D E F G H J N M I)
        (and (= v_14 K) (= v_15 K) (= v_16 K))
      )
      (step_lturn K A B C D E F G H J M L I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (N (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) (v_16 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn K A B C D E F G H J M v_14 I)
        (combined_lturn K A B C D E F G H N M L I)
        (combined_lturn K A B C D E F G H v_15 M L I)
        (step_lturn K A B C D E F G H v_16 N M I)
        (combined_lturn K A B C D E F G H J N M I)
        (and (= v_14 K) (= v_15 K) (= v_16 K))
      )
      (step_lturn K A B C D E F G H J M L I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (N (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) (v_16 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn K A B C D E F G H J M v_14 I)
        (combined_lturn K A B C D E F G H N M L I)
        (combined_lturn K A B C D E F G H v_15 M L I)
        (combined_lturn K A B C D E F G H v_16 N M I)
        (step_lturn K A B C D E F G H J N M I)
        (and (= v_14 K) (= v_15 K) (= v_16 K))
      )
      (step_lturn K A B C D E F G H J M L I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (N (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) (v_16 (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn K A B C D E F G H J M v_14 I)
        (combined_lturn K A B C D E F G H N M L I)
        (combined_lturn K A B C D E F G H v_15 M L I)
        (combined_lturn K A B C D E F G H v_16 N M I)
        (combined_lturn K A B C D E F G H J N M I)
        (and (= v_14 K) (= v_15 K) (= v_16 K))
      )
      (step_lturn K A B C D E F G H J M L I)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (N (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) (v_16 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn K A B C D E F G H J M v_14 I)
        (step_lturn K A B C D E F G H J L M I)
        (combined_lturn K A B C D E F G H N M L I)
        (combined_lturn K A B C D E F G H v_15 M L I)
        (combined_lturn K A B C D E F G H v_16 N M I)
        (combined_lturn K A B C D E F G H J N M I)
        (and (= v_14 K) (= v_15 K) (= v_16 K))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (N (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) (v_16 (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn K A B C D E F G H J M v_14 I)
        (combined_lturn K A B C D E F G H J L M I)
        (combined_lturn K A B C D E F G H N M L I)
        (combined_lturn K A B C D E F G H v_15 M L I)
        (combined_lturn K A B C D E F G H v_16 N M I)
        (combined_lturn K A B C D E F G H J N M I)
        (and (= v_14 K) (= v_15 K) (= v_16 K))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (N (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) (v_16 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn K A B C D E F G H J M v_14 I)
        (combined_lturn K A B C D E F G H J L M I)
        (combined_lturn K A B C D E F G H N M L I)
        (combined_lturn K A B C D E F G H v_15 M L I)
        (combined_lturn K A B C D E F G H v_16 N M I)
        (step_lturn K A B C D E F G H J N M I)
        (and (= v_14 K) (= v_15 K) (= v_16 K))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (N (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) (v_16 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn K A B C D E F G H J M v_14 I)
        (combined_lturn K A B C D E F G H J L M I)
        (combined_lturn K A B C D E F G H N M L I)
        (combined_lturn K A B C D E F G H v_15 M L I)
        (step_lturn K A B C D E F G H v_16 N M I)
        (combined_lturn K A B C D E F G H J N M I)
        (and (= v_14 K) (= v_15 K) (= v_16 K))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (N (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) (v_16 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn K A B C D E F G H J M v_14 I)
        (combined_lturn K A B C D E F G H J L M I)
        (combined_lturn K A B C D E F G H N M L I)
        (step_lturn K A B C D E F G H v_15 M L I)
        (combined_lturn K A B C D E F G H v_16 N M I)
        (combined_lturn K A B C D E F G H J N M I)
        (and (= v_14 K) (= v_15 K) (= v_16 K))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (N (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) (v_16 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn K A B C D E F G H J M v_14 I)
        (combined_lturn K A B C D E F G H J L M I)
        (step_lturn K A B C D E F G H N M L I)
        (combined_lturn K A B C D E F G H v_15 M L I)
        (combined_lturn K A B C D E F G H v_16 N M I)
        (combined_lturn K A B C D E F G H J N M I)
        (and (= v_14 K) (= v_15 K) (= v_16 K))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn J A B C D E F G H v_13 K M I)
        (step_lturn J A B C D E F G H M K L I)
        (combined_lturn J A B C D E F G H v_14 L K I)
        (combined_lturn J A B C D E F G H v_15 M L I)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn J A B C D E F G H v_13 K M I)
        (combined_lturn J A B C D E F G H M K L I)
        (combined_lturn J A B C D E F G H v_14 L K I)
        (combined_lturn J A B C D E F G H v_15 M L I)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn J A B C D E F G H v_13 K M I)
        (combined_lturn J A B C D E F G H M K L I)
        (combined_lturn J A B C D E F G H v_14 L K I)
        (step_lturn J A B C D E F G H v_15 M L I)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn J A B C D E F G H v_13 K M I)
        (combined_lturn J A B C D E F G H M K L I)
        (step_lturn J A B C D E F G H v_14 L K I)
        (combined_lturn J A B C D E F G H v_15 M L I)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn J A B C D E F G H M L K I)
        (step_lturn J A B C D E F G H M K L I)
        true
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn J A B C D E F G H M L K I)
        (combined_lturn J A B C D E F G H M K L I)
        true
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (CHC_COMP_UNUSED Bool) ) 
    (=>
      (and
        CHC_COMP_FALSE
      )
      false
    )
  )
)

(check-sat)
(exit)
