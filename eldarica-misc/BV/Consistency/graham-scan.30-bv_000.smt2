(set-logic HORN)


(declare-fun |combined_lturn__bar| ( (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) ) Bool)
(declare-fun |step_lturn| ( (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) ) Bool)
(declare-fun |lturn| ( (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) ) Bool)
(declare-fun |step_lturn__bar| ( (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) ) Bool)
(declare-fun |CHC_COMP_FALSE| ( ) Bool)
(declare-fun |combined_lturn| ( (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) (_ BitVec 32) ) Bool)

(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) ) 
    (=>
      (and
        (and (bvsle #x00000000 (bvadd #xfffffff6 D E))
     (bvsle #x00000000 (bvadd #xfffffff6 D A))
     (bvsle #x00000000 (bvadd #xfffffff7 E C))
     (bvsle #x00000000 (bvadd #xfffffff7 D C))
     (bvsle #x00000000 (bvadd #xfffffff7 A C))
     (bvsle #x00000000 (bvadd #xfffffff8 E G))
     (bvsle #x00000000 (bvadd #xfffffff8 E B))
     (bvsle #x00000000 (bvadd #xfffffff8 E H))
     (bvsle #x00000000 (bvadd #xfffffff8 D G))
     (bvsle #x00000000 (bvadd #xfffffff8 D B))
     (bvsle #x00000000 (bvadd #xfffffff8 D H))
     (bvsle #x00000000 (bvadd #xfffffff8 A G))
     (bvsle #x00000000 (bvadd #xfffffff8 A B))
     (bvsle #x00000000 (bvadd #xfffffff8 A H))
     (bvsle #x00000000 (bvadd #xfffffff9 G C))
     (bvsle #x00000000 (bvadd #xfffffff9 E L))
     (bvsle #x00000000 (bvadd #xfffffff9 E I))
     (bvsle #x00000000 (bvadd #xfffffff9 D L))
     (bvsle #x00000000 (bvadd #xfffffff9 D I))
     (bvsle #x00000000 (bvadd #xfffffff9 C B))
     (bvsle #x00000000 (bvadd #xfffffff9 C H))
     (bvsle #x00000000 (bvadd #xfffffff9 A L))
     (bvsle #x00000000 (bvadd #xfffffff9 A I))
     (bvsle #x00000000 (bvadd #xfffffffa G B))
     (bvsle #x00000000 (bvadd #xfffffffa G H))
     (bvsle #x00000000 (bvadd #xfffffffa E J))
     (bvsle #x00000000 (bvadd #xfffffffa E F))
     (bvsle #x00000000 (bvadd #xfffffffa D J))
     (bvsle #x00000000 (bvadd #xfffffffa D F))
     (bvsle #x00000000 (bvadd #xfffffffa C L))
     (bvsle #x00000000 (bvadd #xfffffffa C I))
     (bvsle #x00000000 (bvadd #xfffffffa B H))
     (bvsle #x00000000 (bvadd #xfffffffa A J))
     (bvsle #x00000000 (bvadd #xfffffffa A F))
     (bvsle #x00000000 (bvadd #xfffffffc L I))
     (bvsle #x00000000 (bvadd #xfffffffc J H))
     (bvsle #x00000000 (bvadd #xfffffffc G J))
     (bvsle #x00000000 (bvadd #xfffffffc F G))
     (bvsle #x00000000 (bvadd #xfffffffc F B))
     (bvsle #x00000000 (bvadd #xfffffffc F H))
     (bvsle #x00000000 (bvadd #xfffffffc E (bvmul #xffffffff F)))
     (bvsle #x00000000 (bvadd #xfffffffc D (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #xfffffffc D (bvmul #xffffffff F)))
     (bvsle #x00000000 (bvadd #xfffffffc B J))
     (bvsle #x00000000 (bvadd #xfffffffc A (bvmul #xffffffff F)))
     (bvsle #x00000000 (bvadd #xfffffffb L H))
     (bvsle #x00000000 (bvadd #xfffffffb I H))
     (bvsle #x00000000 (bvadd #xfffffffb G L))
     (bvsle #x00000000 (bvadd #xfffffffb G I))
     (bvsle #x00000000 (bvadd #xfffffffb F C))
     (bvsle #x00000000 (bvadd #xfffffffb C J))
     (bvsle #x00000000 (bvadd #xfffffffb B L))
     (bvsle #x00000000 (bvadd #xfffffffb B I))
     (bvsle #x00000000 (bvadd #xfffffffd (bvmul #xffffffff F) C))
     (bvsle #x00000000 (bvadd #xfffffffd L J))
     (bvsle #x00000000 (bvadd #xfffffffd J I))
     (bvsle #x00000000 (bvadd #xfffffffd F L))
     (bvsle #x00000000 (bvadd #xfffffffd F I))
     (bvsle #x00000000 (bvadd #xfffffffd E (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #xfffffffd D (bvmul #xffffffff L)))
     (bvsle #x00000000 (bvadd #xfffffffd D (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffd C (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #xfffffffd A (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff J) H))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff F) G))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff F) B))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff F) H))
     (bvsle #x00000000 (bvadd #xfffffffe G (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #xfffffffe F J))
     (bvsle #x00000000 (bvadd #xfffffffe E (bvmul #xffffffff L)))
     (bvsle #x00000000 (bvadd #xfffffffe E (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffe D (bvmul #xffffffff G)))
     (bvsle #x00000000 (bvadd #xfffffffe D (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #xfffffffe D (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xfffffffe C (bvmul #xffffffff L)))
     (bvsle #x00000000 (bvadd #xfffffffe C (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffe B (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #xfffffffe A (bvmul #xffffffff L)))
     (bvsle #x00000000 (bvadd #xfffffffe A (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff J) I))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff I) H))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff G) C))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff F) L))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff F) I))
     (bvsle #x00000000 (bvadd #xffffffff L (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #xffffffff G (bvmul #xffffffff L)))
     (bvsle #x00000000 (bvadd #xffffffff G (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xffffffff E (bvmul #xffffffff G)))
     (bvsle #x00000000 (bvadd #xffffffff E (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #xffffffff E (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xffffffff D (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd #xffffffff C (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #xffffffff C (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xffffffff B (bvmul #xffffffff L)))
     (bvsle #x00000000 (bvadd #xffffffff B (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xffffffff A (bvmul #xffffffff G)))
     (bvsle #x00000000 (bvadd #xffffffff A (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #xffffffff A (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #x00000003 (bvmul #xffffffff D) L))
     (bvsle #x00000000 (bvadd #x00000002 (bvmul #xffffffff D) G))
     (bvsle #x00000000 (bvadd #x00000002 (bvmul #xffffffff D) B))
     (bvsle #x00000000 (bvadd #x00000002 (bvmul #xffffffff C) L))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff G) L))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff D) E))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff D) C))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff D) A))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff C) B))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff B) L))
     (bvsle #x00000000 (bvadd #x00000001 L (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #x00000001 J (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #x00000001 G (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd #xfffffffc C))
     (bvsle #x00000000 (bvadd #xfffffffb E))
     (bvsle #x00000000 (bvadd #xfffffffb D))
     (bvsle #x00000000 (bvadd #xfffffffb A))
     (bvsle #x00000000 (bvadd #xfffffffd G))
     (bvsle #x00000000 (bvadd #xfffffffd B))
     (bvsle #x00000000 (bvadd #xfffffffd H))
     (bvsle #x00000000 (bvadd #xfffffffe L))
     (bvsle #x00000000 (bvadd #xfffffffe I))
     (bvsle #x00000000 (bvadd #xffffffff J))
     (bvsle #x00000000 (bvadd #xffffffff F))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff G) B))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff F) J))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff E) A))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff F)))
     (bvsle #x00000000 (bvadd L (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd G (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd G (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd E (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd E (bvmul #xffffffff A)))
     (bvsle #x00000000 (bvadd B (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd A (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd #xfffffff6 E A)))
      )
      (lturn A L B C D E F G H I J K)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) ) 
    (=>
      (and
        (and (bvsle #x00000000 (bvadd #xfffffff7 D E))
     (bvsle #x00000000 (bvadd #xfffffff7 D A))
     (bvsle #x00000000 (bvadd #xfffffff8 E C))
     (bvsle #x00000000 (bvadd #xfffffff8 E H))
     (bvsle #x00000000 (bvadd #xfffffff8 A C))
     (bvsle #x00000000 (bvadd #xfffffff8 A H))
     (bvsle #x00000000 (bvadd #xfffffff9 E I))
     (bvsle #x00000000 (bvadd #xfffffff9 E G))
     (bvsle #x00000000 (bvadd #xfffffff9 E B))
     (bvsle #x00000000 (bvadd #xfffffff9 D C))
     (bvsle #x00000000 (bvadd #xfffffff9 D H))
     (bvsle #x00000000 (bvadd #xfffffff9 A I))
     (bvsle #x00000000 (bvadd #xfffffff9 A G))
     (bvsle #x00000000 (bvadd #xfffffff9 A B))
     (bvsle #x00000000 (bvadd #xfffffffa E L))
     (bvsle #x00000000 (bvadd #xfffffffa E J))
     (bvsle #x00000000 (bvadd #xfffffffa E F))
     (bvsle #x00000000 (bvadd #xfffffffa D I))
     (bvsle #x00000000 (bvadd #xfffffffa D G))
     (bvsle #x00000000 (bvadd #xfffffffa D B))
     (bvsle #x00000000 (bvadd #xfffffffa C H))
     (bvsle #x00000000 (bvadd #xfffffffa A L))
     (bvsle #x00000000 (bvadd #xfffffffa A J))
     (bvsle #x00000000 (bvadd #xfffffffa A F))
     (bvsle #x00000000 (bvadd #xfffffffc L H))
     (bvsle #x00000000 (bvadd #xfffffffc J H))
     (bvsle #x00000000 (bvadd #xfffffffc G I))
     (bvsle #x00000000 (bvadd #xfffffffc G B))
     (bvsle #x00000000 (bvadd #xfffffffc F C))
     (bvsle #x00000000 (bvadd #xfffffffc F H))
     (bvsle #x00000000 (bvadd #xfffffffc E (bvmul #xffffffff F)))
     (bvsle #x00000000 (bvadd #xfffffffc C L))
     (bvsle #x00000000 (bvadd #xfffffffc C J))
     (bvsle #x00000000 (bvadd #xfffffffc B I))
     (bvsle #x00000000 (bvadd #xfffffffc A (bvmul #xffffffff F)))
     (bvsle #x00000000 (bvadd #xfffffffb I H))
     (bvsle #x00000000 (bvadd #xfffffffb G C))
     (bvsle #x00000000 (bvadd #xfffffffb G H))
     (bvsle #x00000000 (bvadd #xfffffffb D L))
     (bvsle #x00000000 (bvadd #xfffffffb D J))
     (bvsle #x00000000 (bvadd #xfffffffb D F))
     (bvsle #x00000000 (bvadd #xfffffffb C I))
     (bvsle #x00000000 (bvadd #xfffffffb C B))
     (bvsle #x00000000 (bvadd #xfffffffb B H))
     (bvsle #x00000000 (bvadd #xfffffffd L I))
     (bvsle #x00000000 (bvadd #xfffffffd J I))
     (bvsle #x00000000 (bvadd #xfffffffd G L))
     (bvsle #x00000000 (bvadd #xfffffffd G J))
     (bvsle #x00000000 (bvadd #xfffffffd F I))
     (bvsle #x00000000 (bvadd #xfffffffd F G))
     (bvsle #x00000000 (bvadd #xfffffffd F B))
     (bvsle #x00000000 (bvadd #xfffffffd D (bvmul #xffffffff L)))
     (bvsle #x00000000 (bvadd #xfffffffd D (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #xfffffffd D (bvmul #xffffffff F)))
     (bvsle #x00000000 (bvadd #xfffffffd B L))
     (bvsle #x00000000 (bvadd #xfffffffd B J))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff L) H))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff J) H))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff F) C))
     (bvsle #x00000000 (bvadd #xfffffffe (bvmul #xffffffff F) H))
     (bvsle #x00000000 (bvadd #xfffffffe L J))
     (bvsle #x00000000 (bvadd #xfffffffe F L))
     (bvsle #x00000000 (bvadd #xfffffffe F J))
     (bvsle #x00000000 (bvadd #xfffffffe E (bvmul #xffffffff L)))
     (bvsle #x00000000 (bvadd #xfffffffe E (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #xfffffffe D (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffe D (bvmul #xffffffff G)))
     (bvsle #x00000000 (bvadd #xfffffffe D (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #xfffffffe C (bvmul #xffffffff L)))
     (bvsle #x00000000 (bvadd #xfffffffe C (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #xfffffffe A (bvmul #xffffffff L)))
     (bvsle #x00000000 (bvadd #xfffffffe A (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff L) I))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff J) I))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff I) H))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff G) C))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff G) H))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff F) I))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff F) G))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff F) B))
     (bvsle #x00000000 (bvadd #xffffffff (bvmul #xffffffff B) H))
     (bvsle #x00000000 (bvadd #xffffffff G (bvmul #xffffffff L)))
     (bvsle #x00000000 (bvadd #xffffffff G (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #xffffffff E (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xffffffff E (bvmul #xffffffff G)))
     (bvsle #x00000000 (bvadd #xffffffff E (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #xffffffff D (bvmul #xffffffff C)))
     (bvsle #x00000000 (bvadd #xffffffff D (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd #xffffffff C (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xffffffff C (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #xffffffff B (bvmul #xffffffff L)))
     (bvsle #x00000000 (bvadd #xffffffff B (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd #xffffffff A (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xffffffff A (bvmul #xffffffff G)))
     (bvsle #x00000000 (bvadd #xffffffff A (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff G) L))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff G) J))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff D) C))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff D) H))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff B) L))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff B) J))
     (bvsle #x00000000 (bvadd #x00000001 L (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #x00000001 J (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffffc D))
     (bvsle #x00000000 (bvadd #xfffffffb E))
     (bvsle #x00000000 (bvadd #xfffffffb A))
     (bvsle #x00000000 (bvadd #xfffffffd C))
     (bvsle #x00000000 (bvadd #xfffffffd H))
     (bvsle #x00000000 (bvadd #xfffffffe I))
     (bvsle #x00000000 (bvadd #xfffffffe G))
     (bvsle #x00000000 (bvadd #xfffffffe B))
     (bvsle #x00000000 (bvadd #xffffffff L))
     (bvsle #x00000000 (bvadd #xffffffff J))
     (bvsle #x00000000 (bvadd #xffffffff F))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff L) J))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff G) I))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff G) B))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff F) L))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff F) J))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff E) A))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff C) H))
     (bvsle #x00000000 (bvadd (bvmul #xffffffff B) I))
     (bvsle #x00000000 (bvadd #x00000001 (bvmul #xffffffff F)))
     (bvsle #x00000000 (bvadd L (bvmul #xffffffff J)))
     (bvsle #x00000000 (bvadd G (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd G (bvmul #xffffffff B)))
     (bvsle #x00000000 (bvadd E (bvmul #xffffffff A)))
     (bvsle #x00000000 (bvadd C (bvmul #xffffffff H)))
     (bvsle #x00000000 (bvadd B (bvmul #xffffffff I)))
     (bvsle #x00000000 (bvadd #xfffffff6 E A)))
      )
      (step_lturn__bar A L B C D E F G H I J K)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) ) 
    (=>
      (and
        (lturn A L B C D E F G H I J K)
        true
      )
      (combined_lturn A L B C D E F G H I J K)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn A L B C D E F G H I J K)
        true
      )
      (combined_lturn A L B C D E F G H I J K)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn__bar A L B C D E F G H I J K)
        true
      )
      (combined_lturn__bar A L B C D E F G H I J K)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn A I B C D E F G K J L H)
        true
      )
      (lturn A I B C D E F G L K J H)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn__bar A I B C D E F G L J K H)
        true
      )
      (lturn A I B C D E F G L K J H)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (v_12 (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A I B C D E F G v_12 J L H)
        (step_lturn A I B C D E F G v_13 K J H)
        (combined_lturn A I B C D E F G v_14 L K H)
        (and (= v_12 I) (= v_13 I) (= v_14 I))
      )
      (lturn A I B C D E F G L K J H)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (v_12 (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A I B C D E F G v_12 J L H)
        (combined_lturn A I B C D E F G v_13 K J H)
        (step_lturn A I B C D E F G v_14 L K H)
        (and (= v_12 I) (= v_13 I) (= v_14 I))
      )
      (lturn A I B C D E F G L K J H)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (v_12 (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn A I B C D E F G v_12 J L H)
        (combined_lturn A I B C D E F G v_13 K J H)
        (combined_lturn A I B C D E F G v_14 L K H)
        (and (= v_12 I) (= v_13 I) (= v_14 I))
      )
      (lturn A I B C D E F G L K J H)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A J B C D E F G I L v_13 H)
        (step_lturn A J B C D E F G M L K H)
        (combined_lturn A J B C D E F G v_14 L K H)
        (combined_lturn A J B C D E F G v_15 M L H)
        (combined_lturn A J B C D E F G I M L H)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      (lturn A J B C D E F G I L K H)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A J B C D E F G I L v_13 H)
        (combined_lturn A J B C D E F G M L K H)
        (step_lturn A J B C D E F G v_14 L K H)
        (combined_lturn A J B C D E F G v_15 M L H)
        (combined_lturn A J B C D E F G I M L H)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      (lturn A J B C D E F G I L K H)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A J B C D E F G I L v_13 H)
        (combined_lturn A J B C D E F G M L K H)
        (combined_lturn A J B C D E F G v_14 L K H)
        (step_lturn A J B C D E F G v_15 M L H)
        (combined_lturn A J B C D E F G I M L H)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      (lturn A J B C D E F G I L K H)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A J B C D E F G I L v_13 H)
        (combined_lturn A J B C D E F G M L K H)
        (combined_lturn A J B C D E F G v_14 L K H)
        (combined_lturn A J B C D E F G v_15 M L H)
        (step_lturn A J B C D E F G I M L H)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      (lturn A J B C D E F G I L K H)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn A J B C D E F G I L v_13 H)
        (combined_lturn A J B C D E F G M L K H)
        (combined_lturn A J B C D E F G v_14 L K H)
        (combined_lturn A J B C D E F G v_15 M L H)
        (combined_lturn A J B C D E F G I M L H)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      (lturn A J B C D E F G I L K H)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn A I B C D E F G K J L H)
        true
      )
      (step_lturn A I B C D E F G L K J H)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn__bar A I B C D E F G L J K H)
        true
      )
      (step_lturn A I B C D E F G L K J H)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (v_12 (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A I B C D E F G v_12 J L H)
        (step_lturn A I B C D E F G v_13 K J H)
        (combined_lturn A I B C D E F G v_14 L K H)
        (and (= v_12 I) (= v_13 I) (= v_14 I))
      )
      (step_lturn A I B C D E F G L K J H)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (v_12 (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A I B C D E F G v_12 J L H)
        (combined_lturn A I B C D E F G v_13 K J H)
        (step_lturn A I B C D E F G v_14 L K H)
        (and (= v_12 I) (= v_13 I) (= v_14 I))
      )
      (step_lturn A I B C D E F G L K J H)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (v_12 (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn A I B C D E F G v_12 J L H)
        (combined_lturn A I B C D E F G v_13 K J H)
        (combined_lturn A I B C D E F G v_14 L K H)
        (and (= v_12 I) (= v_13 I) (= v_14 I))
      )
      (step_lturn A I B C D E F G L K J H)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A J B C D E F G I L v_13 H)
        (step_lturn A J B C D E F G M L K H)
        (combined_lturn A J B C D E F G v_14 L K H)
        (combined_lturn A J B C D E F G v_15 M L H)
        (combined_lturn A J B C D E F G I M L H)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      (step_lturn A J B C D E F G I L K H)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A J B C D E F G I L v_13 H)
        (combined_lturn A J B C D E F G M L K H)
        (step_lturn A J B C D E F G v_14 L K H)
        (combined_lturn A J B C D E F G v_15 M L H)
        (combined_lturn A J B C D E F G I M L H)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      (step_lturn A J B C D E F G I L K H)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A J B C D E F G I L v_13 H)
        (combined_lturn A J B C D E F G M L K H)
        (combined_lturn A J B C D E F G v_14 L K H)
        (step_lturn A J B C D E F G v_15 M L H)
        (combined_lturn A J B C D E F G I M L H)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      (step_lturn A J B C D E F G I L K H)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A J B C D E F G I L v_13 H)
        (combined_lturn A J B C D E F G M L K H)
        (combined_lturn A J B C D E F G v_14 L K H)
        (combined_lturn A J B C D E F G v_15 M L H)
        (step_lturn A J B C D E F G I M L H)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      (step_lturn A J B C D E F G I L K H)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn A J B C D E F G I L v_13 H)
        (combined_lturn A J B C D E F G M L K H)
        (combined_lturn A J B C D E F G v_14 L K H)
        (combined_lturn A J B C D E F G v_15 M L H)
        (combined_lturn A J B C D E F G I M L H)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      (step_lturn A J B C D E F G I L K H)
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A J B C D E F G I L v_13 H)
        (step_lturn A J B C D E F G I K L H)
        (combined_lturn A J B C D E F G M L K H)
        (combined_lturn A J B C D E F G v_14 L K H)
        (combined_lturn A J B C D E F G v_15 M L H)
        (combined_lturn A J B C D E F G I M L H)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn A J B C D E F G I L v_13 H)
        (combined_lturn A J B C D E F G I K L H)
        (combined_lturn A J B C D E F G M L K H)
        (combined_lturn A J B C D E F G v_14 L K H)
        (combined_lturn A J B C D E F G v_15 M L H)
        (combined_lturn A J B C D E F G I M L H)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A J B C D E F G I L v_13 H)
        (combined_lturn A J B C D E F G I K L H)
        (combined_lturn A J B C D E F G M L K H)
        (combined_lturn A J B C D E F G v_14 L K H)
        (combined_lturn A J B C D E F G v_15 M L H)
        (step_lturn A J B C D E F G I M L H)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A J B C D E F G I L v_13 H)
        (combined_lturn A J B C D E F G I K L H)
        (combined_lturn A J B C D E F G M L K H)
        (combined_lturn A J B C D E F G v_14 L K H)
        (step_lturn A J B C D E F G v_15 M L H)
        (combined_lturn A J B C D E F G I M L H)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A J B C D E F G I L v_13 H)
        (combined_lturn A J B C D E F G I K L H)
        (combined_lturn A J B C D E F G M L K H)
        (step_lturn A J B C D E F G v_14 L K H)
        (combined_lturn A J B C D E F G v_15 M L H)
        (combined_lturn A J B C D E F G I M L H)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (M (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) (v_15 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A J B C D E F G I L v_13 H)
        (combined_lturn A J B C D E F G I K L H)
        (step_lturn A J B C D E F G M L K H)
        (combined_lturn A J B C D E F G v_14 L K H)
        (combined_lturn A J B C D E F G v_15 M L H)
        (combined_lturn A J B C D E F G I M L H)
        (and (= v_13 J) (= v_14 J) (= v_15 J))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (v_12 (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A I B C D E F G v_12 J L H)
        (step_lturn A I B C D E F G L J K H)
        (combined_lturn A I B C D E F G v_13 K J H)
        (combined_lturn A I B C D E F G v_14 L K H)
        (and (= v_12 I) (= v_13 I) (= v_14 I))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (v_12 (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn A I B C D E F G v_12 J L H)
        (combined_lturn A I B C D E F G L J K H)
        (combined_lturn A I B C D E F G v_13 K J H)
        (combined_lturn A I B C D E F G v_14 L K H)
        (and (= v_12 I) (= v_13 I) (= v_14 I))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (v_12 (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A I B C D E F G v_12 J L H)
        (combined_lturn A I B C D E F G L J K H)
        (combined_lturn A I B C D E F G v_13 K J H)
        (step_lturn A I B C D E F G v_14 L K H)
        (and (= v_12 I) (= v_13 I) (= v_14 I))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) (v_12 (_ BitVec 32)) (v_13 (_ BitVec 32)) (v_14 (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A I B C D E F G v_12 J L H)
        (combined_lturn A I B C D E F G L J K H)
        (step_lturn A I B C D E F G v_13 K J H)
        (combined_lturn A I B C D E F G v_14 L K H)
        (and (= v_12 I) (= v_13 I) (= v_14 I))
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn A I B C D E F G L K J H)
        (step_lturn A I B C D E F G L J K H)
        true
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn A I B C D E F G L K J H)
        (combined_lturn A I B C D E F G L J K H)
        true
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) ) 
    (=>
      (and
        (combined_lturn__bar A I B C D E F G L K J H)
        (step_lturn A I B C D E F G L K J H)
        true
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (A (_ BitVec 32)) (B (_ BitVec 32)) (C (_ BitVec 32)) (D (_ BitVec 32)) (E (_ BitVec 32)) (F (_ BitVec 32)) (G (_ BitVec 32)) (H (_ BitVec 32)) (I (_ BitVec 32)) (J (_ BitVec 32)) (K (_ BitVec 32)) (L (_ BitVec 32)) ) 
    (=>
      (and
        (step_lturn__bar A I B C D E F G L K J H)
        (combined_lturn A I B C D E F G L K J H)
        true
      )
      CHC_COMP_FALSE
    )
  )
)
(assert
  (forall ( (CHC_COMP_UNUSED Bool) ) 
    (=>
      (and
        CHC_COMP_FALSE
      )
      false
    )
  )
)

(check-sat)
(exit)
